{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJHlxbR5kEe-"
      },
      "source": [
        "# Mixed Logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQbZt7CVh8f_",
        "outputId": "b823e80f-fd47-4dd1-8656-3fd0d6a1e26a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import jax\n",
        "\n",
        "from jaxlogit.mixed_logit import MixedLogit, ConfigData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  64bit precision\n",
        "jax.config.update(\"jax_enable_x64\", True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP77ezqVfvRI"
      },
      "source": [
        "## Swissmetro Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOWB3Lffg5Qc"
      },
      "source": [
        "\n",
        "The swissmetro dataset contains stated-preferences for three alternative transportation modes that include car, train and a newly introduced mode: the swissmetro. This dataset is commonly used for estimation examples with the `Biogeme` and `PyLogit` packages. The dataset is available at http://transp-or.epfl.ch/data/swissmetro.dat and [Bierlaire et. al., (2001)](https://transp-or.epfl.ch/documents/proceedings/BierAxhaAbay01.pdf) provides a detailed discussion of the data as wells as its context and collection process. The explanatory variables in this example include the travel time (`TT`) and cost `CO` for each of the three alternative modes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4No84MAeFOM"
      },
      "source": [
        "### Read data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEzmVzYDdLS8"
      },
      "source": [
        "The dataset is imported to the Python environment using `pandas`. Then, two types of samples, ones with a trip purpose different to commute or business and ones with an unknown choice, are filtered out. The original dataset contains 10,729 records, but after filtering, 6,768 records remain for following analysis. Finally, a new column that uniquely identifies each sample is added to the dataframe and the `CHOICE` column, which originally contains a numerical coding of the choices, is mapped to a description that is consistent with the alternatives in the column names. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "4jqERhnWhGCc",
        "outputId": "6bbdca2a-1670-4836-c0d5-d16915ee9597"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GROUP</th>\n",
              "      <th>SURVEY</th>\n",
              "      <th>SP</th>\n",
              "      <th>ID</th>\n",
              "      <th>PURPOSE</th>\n",
              "      <th>FIRST</th>\n",
              "      <th>TICKET</th>\n",
              "      <th>WHO</th>\n",
              "      <th>LUGGAGE</th>\n",
              "      <th>AGE</th>\n",
              "      <th>...</th>\n",
              "      <th>TRAIN_CO</th>\n",
              "      <th>TRAIN_HE</th>\n",
              "      <th>SM_TT</th>\n",
              "      <th>SM_CO</th>\n",
              "      <th>SM_HE</th>\n",
              "      <th>SM_SEATS</th>\n",
              "      <th>CAR_TT</th>\n",
              "      <th>CAR_CO</th>\n",
              "      <th>CHOICE</th>\n",
              "      <th>custom_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>48</td>\n",
              "      <td>120</td>\n",
              "      <td>63</td>\n",
              "      <td>52</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>117</td>\n",
              "      <td>65</td>\n",
              "      <td>SM</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>48</td>\n",
              "      <td>30</td>\n",
              "      <td>60</td>\n",
              "      <td>49</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>117</td>\n",
              "      <td>84</td>\n",
              "      <td>SM</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>48</td>\n",
              "      <td>60</td>\n",
              "      <td>67</td>\n",
              "      <td>58</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>117</td>\n",
              "      <td>52</td>\n",
              "      <td>SM</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>40</td>\n",
              "      <td>30</td>\n",
              "      <td>63</td>\n",
              "      <td>52</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>72</td>\n",
              "      <td>52</td>\n",
              "      <td>SM</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>36</td>\n",
              "      <td>60</td>\n",
              "      <td>63</td>\n",
              "      <td>42</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>90</td>\n",
              "      <td>84</td>\n",
              "      <td>SM</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8446</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>939</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>13</td>\n",
              "      <td>30</td>\n",
              "      <td>50</td>\n",
              "      <td>17</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>64</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>6763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8447</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>939</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>53</td>\n",
              "      <td>16</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "      <td>80</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>6764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8448</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>939</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>16</td>\n",
              "      <td>60</td>\n",
              "      <td>50</td>\n",
              "      <td>16</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "      <td>64</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>6765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8449</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>939</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>16</td>\n",
              "      <td>30</td>\n",
              "      <td>53</td>\n",
              "      <td>17</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "      <td>104</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>6766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8450</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>939</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>13</td>\n",
              "      <td>60</td>\n",
              "      <td>53</td>\n",
              "      <td>21</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>80</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>6767</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6768 rows × 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      GROUP  SURVEY  SP   ID  PURPOSE  FIRST  TICKET  WHO  LUGGAGE  AGE  ...  \\\n",
              "0         2       0   1    1        1      0       1    1        0    3  ...   \n",
              "1         2       0   1    1        1      0       1    1        0    3  ...   \n",
              "2         2       0   1    1        1      0       1    1        0    3  ...   \n",
              "3         2       0   1    1        1      0       1    1        0    3  ...   \n",
              "4         2       0   1    1        1      0       1    1        0    3  ...   \n",
              "...     ...     ...  ..  ...      ...    ...     ...  ...      ...  ...  ...   \n",
              "8446      3       1   1  939        3      1       7    3        1    5  ...   \n",
              "8447      3       1   1  939        3      1       7    3        1    5  ...   \n",
              "8448      3       1   1  939        3      1       7    3        1    5  ...   \n",
              "8449      3       1   1  939        3      1       7    3        1    5  ...   \n",
              "8450      3       1   1  939        3      1       7    3        1    5  ...   \n",
              "\n",
              "      TRAIN_CO  TRAIN_HE  SM_TT  SM_CO  SM_HE  SM_SEATS  CAR_TT  CAR_CO  \\\n",
              "0           48       120     63     52     20         0     117      65   \n",
              "1           48        30     60     49     10         0     117      84   \n",
              "2           48        60     67     58     30         0     117      52   \n",
              "3           40        30     63     52     20         0      72      52   \n",
              "4           36        60     63     42     20         0      90      84   \n",
              "...        ...       ...    ...    ...    ...       ...     ...     ...   \n",
              "8446        13        30     50     17     30         0     130      64   \n",
              "8447        12        30     53     16     10         0      80      80   \n",
              "8448        16        60     50     16     20         0      80      64   \n",
              "8449        16        30     53     17     30         0      80     104   \n",
              "8450        13        60     53     21     30         0     100      80   \n",
              "\n",
              "      CHOICE  custom_id  \n",
              "0         SM          0  \n",
              "1         SM          1  \n",
              "2         SM          2  \n",
              "3         SM          3  \n",
              "4         SM          4  \n",
              "...      ...        ...  \n",
              "8446   TRAIN       6763  \n",
              "8447   TRAIN       6764  \n",
              "8448   TRAIN       6765  \n",
              "8449   TRAIN       6766  \n",
              "8450   TRAIN       6767  \n",
              "\n",
              "[6768 rows x 29 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_wide = pd.read_table(\"http://transp-or.epfl.ch/data/swissmetro.dat\", sep='\\t')\n",
        "\n",
        "# Keep only observations for commute and business purposes that contain known choices\n",
        "df_wide = df_wide[(df_wide['PURPOSE'].isin([1, 3]) & (df_wide['CHOICE'] != 0))]\n",
        "\n",
        "df_wide['custom_id'] = np.arange(len(df_wide))  # Add unique identifier\n",
        "df_wide['CHOICE'] = df_wide['CHOICE'].map({1: 'TRAIN', 2:'SM', 3: 'CAR'})\n",
        "df_wide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GRMhgM2eIPz"
      },
      "source": [
        "### Reshape data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9OxW-yNhcal"
      },
      "source": [
        "The imported dataframe is in wide format, and it needs to be reshaped to long format for processing by `xlogit`, which offers the convenient `wide_to_long` utility for this reshaping process. The user needs to specify the column that uniquely identifies each sample, the names of the alternatives, the columns that vary across alternatives, and whether the alternative names are a prefix or suffix of the column names. Additionally, the user can specify a value (`empty_val`) to be used by default when an alternative is not available for a certain variable. Additional usage examples for the `wide_to_long` function are available in xlogit's documentation at https://xlogit.readthedocs.io/en/latest/notebooks/convert_data_wide_to_long.html. Also, details about the function parameters are available at the [API reference ](https://xlogit.readthedocs.io/en/latest/api/utils.html#xlogit.utils.wide_to_long)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "1KM-BvFvhWed",
        "outputId": "33a6bacf-9674-4fec-eeca-90ab763a3308"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>custom_id</th>\n",
              "      <th>alt</th>\n",
              "      <th>TT</th>\n",
              "      <th>CO</th>\n",
              "      <th>HE</th>\n",
              "      <th>AV</th>\n",
              "      <th>SEATS</th>\n",
              "      <th>GROUP</th>\n",
              "      <th>SURVEY</th>\n",
              "      <th>SP</th>\n",
              "      <th>...</th>\n",
              "      <th>TICKET</th>\n",
              "      <th>WHO</th>\n",
              "      <th>LUGGAGE</th>\n",
              "      <th>AGE</th>\n",
              "      <th>MALE</th>\n",
              "      <th>INCOME</th>\n",
              "      <th>GA</th>\n",
              "      <th>ORIGIN</th>\n",
              "      <th>DEST</th>\n",
              "      <th>CHOICE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>112</td>\n",
              "      <td>48</td>\n",
              "      <td>120</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>SM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>SM</td>\n",
              "      <td>63</td>\n",
              "      <td>52</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>SM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>CAR</td>\n",
              "      <td>117</td>\n",
              "      <td>65</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>SM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>103</td>\n",
              "      <td>48</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>SM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>SM</td>\n",
              "      <td>60</td>\n",
              "      <td>49</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>SM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20299</th>\n",
              "      <td>6766</td>\n",
              "      <td>SM</td>\n",
              "      <td>53</td>\n",
              "      <td>17</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>TRAIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20300</th>\n",
              "      <td>6766</td>\n",
              "      <td>CAR</td>\n",
              "      <td>80</td>\n",
              "      <td>104</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>TRAIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20301</th>\n",
              "      <td>6767</td>\n",
              "      <td>TRAIN</td>\n",
              "      <td>108</td>\n",
              "      <td>13</td>\n",
              "      <td>60</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>TRAIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20302</th>\n",
              "      <td>6767</td>\n",
              "      <td>SM</td>\n",
              "      <td>53</td>\n",
              "      <td>21</td>\n",
              "      <td>30</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>TRAIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20303</th>\n",
              "      <td>6767</td>\n",
              "      <td>CAR</td>\n",
              "      <td>100</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>TRAIN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20304 rows × 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       custom_id    alt   TT   CO   HE  AV  SEATS  GROUP  SURVEY  SP  ...  \\\n",
              "0              0  TRAIN  112   48  120   1      0      2       0   1  ...   \n",
              "1              0     SM   63   52   20   1      0      2       0   1  ...   \n",
              "2              0    CAR  117   65    0   1      0      2       0   1  ...   \n",
              "3              1  TRAIN  103   48   30   1      0      2       0   1  ...   \n",
              "4              1     SM   60   49   10   1      0      2       0   1  ...   \n",
              "...          ...    ...  ...  ...  ...  ..    ...    ...     ...  ..  ...   \n",
              "20299       6766     SM   53   17   30   1      0      3       1   1  ...   \n",
              "20300       6766    CAR   80  104    0   1      0      3       1   1  ...   \n",
              "20301       6767  TRAIN  108   13   60   1      0      3       1   1  ...   \n",
              "20302       6767     SM   53   21   30   1      0      3       1   1  ...   \n",
              "20303       6767    CAR  100   80    0   1      0      3       1   1  ...   \n",
              "\n",
              "       TICKET  WHO  LUGGAGE  AGE  MALE  INCOME  GA  ORIGIN  DEST  CHOICE  \n",
              "0           1    1        0    3     0       2   0       2     1      SM  \n",
              "1           1    1        0    3     0       2   0       2     1      SM  \n",
              "2           1    1        0    3     0       2   0       2     1      SM  \n",
              "3           1    1        0    3     0       2   0       2     1      SM  \n",
              "4           1    1        0    3     0       2   0       2     1      SM  \n",
              "...       ...  ...      ...  ...   ...     ...  ..     ...   ...     ...  \n",
              "20299       7    3        1    5     1       2   0       1     2   TRAIN  \n",
              "20300       7    3        1    5     1       2   0       1     2   TRAIN  \n",
              "20301       7    3        1    5     1       2   0       1     2   TRAIN  \n",
              "20302       7    3        1    5     1       2   0       1     2   TRAIN  \n",
              "20303       7    3        1    5     1       2   0       1     2   TRAIN  \n",
              "\n",
              "[20304 rows x 23 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from jaxlogit.utils import wide_to_long\n",
        "\n",
        "df = wide_to_long(df_wide, id_col='custom_id', alt_name='alt', sep='_',\n",
        "                  alt_list=['TRAIN', 'SM', 'CAR'], empty_val=0,\n",
        "                  varying=['TT', 'CO', 'HE', 'AV', 'SEATS'], alt_is_prefix=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhLjuzaSeVjE"
      },
      "source": [
        "### Create model specification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgJP2WdQeXiY"
      },
      "source": [
        "Following the reshaping, users can create or update the dataset's columns in order to accommodate their model specification needs, if necessary. The code below shows how the columns `ASC_TRAIN` and `ASC_CAR` were created to incorporate alternative-specific constants in the model. In addition, the example illustrates an effective way of establishing variable interactions (e.g., trip costs for commuters with an annual pass) by updating existing columns conditional on values of other columns. Although apparently simple, column operations provide users with an intuitive and highly-flexible mechanism to incorporate model specification aspects, such as variable transformations, interactions, and alternative specific coefficients and constants. By operating the dataframe columns, any utility specification can be accommodated in `xlogit`. As shown in [this specification example](https://xlogit.readthedocs.io/en/latest/notebooks/multinomial_model.html#Create-model-specification), highly-flexible utility specifications can be modeled in `xlogit` by operating the dataframe columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsSu2jqKeoz-"
      },
      "outputs": [],
      "source": [
        "df['ASC_TRAIN'] = np.ones(len(df))*(df['alt'] == 'TRAIN')\n",
        "df['ASC_CAR'] = np.ones(len(df))*(df['alt'] == 'CAR')\n",
        "df['TT'], df['CO'] = df['TT']/100, df['CO']/100  # Scale variables\n",
        "annual_pass = (df['GA'] == 1) & (df['alt'].isin(['TRAIN', 'SM']))\n",
        "df.loc[annual_pass, 'CO'] = 0  # Cost zero for pass holders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgjEi8QLexj6"
      },
      "source": [
        "### Estimate model parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzuSO2UBe99t"
      },
      "source": [
        "The `fit` method estimates the model by taking as input the data from the previous step along with additional specification criteria, such as the distribution of the random parameters (`randvars`), the number of random draws (`n_draws`), and the availability of alternatives for the choice situations (`avail`). We set the optimization method as `L-BFGS-B` as this is a robust routine that usually helps solve convergence issues.  Once the estimation routine is completed, the `summary` method can be used to display the estimation results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dctkrAPBez4T",
        "outputId": "eaacd08d-fec7-4b8d-b0dd-757e4f3eac15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-14 00:29:34,451 INFO jaxlogit.mixed_logit: Starting data preparation, including generation of 1500 random draws for each random variable and observation.\n",
            "INFO:2025-07-14 00:29:34,528:jax._src.xla_bridge:752: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
            "2025-07-14 00:29:34,528 INFO jax._src.xla_bridge: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-14 00:29:37,022 INFO jaxlogit.mixed_logit: Data contains 752 panels, using segment_sum for panel-wise log-likelihood.\n",
            "2025-07-14 00:29:37,023 INFO jaxlogit.mixed_logit: Shape of draws: (6768, 1, 1500), number of draws: 1500\n",
            "2025-07-14 00:29:37,024 INFO jaxlogit.mixed_logit: Shape of Xdf: (6768, 2, 3), shape of Xdr: (6768, 2, 1)\n",
            "2025-07-14 00:29:37,026 INFO jaxlogit.mixed_logit: Compiling log-likelihood function.\n",
            "2025-07-14 00:29:37,459 INFO jaxlogit.mixed_logit: Compilation finished, init neg_loglike = 6260.71, params= [(np.str_('ASC_CAR'), Array(0.1, dtype=float64)), (np.str_('ASC_TRAIN'), Array(0.1, dtype=float64)), (np.str_('CO'), Array(0.1, dtype=float64)), (np.str_('TT'), Array(0.1, dtype=float64)), (np.str_('sd.TT'), Array(0.1, dtype=float64))]\n",
            "2025-07-14 00:29:37,461 INFO jaxlogit._optimize: Running minimization with method trust-region\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss on this step: 6260.709404860119, Loss on the last accepted step: 0.0, Step size: 1.0\n",
            "Loss on this step: 205471.97895501574, Loss on the last accepted step: 6260.709404860119, Step size: 0.25\n",
            "Loss on this step: 123910.46308477379, Loss on the last accepted step: 6260.709404860119, Step size: 0.0625\n",
            "Loss on this step: 45412.46942728786, Loss on the last accepted step: 6260.709404860119, Step size: 0.015625\n",
            "Loss on this step: 13605.640755086313, Loss on the last accepted step: 6260.709404860119, Step size: 0.00390625\n",
            "Loss on this step: 5957.889047455692, Loss on the last accepted step: 6260.709404860119, Step size: 0.00390625\n",
            "Loss on this step: 4662.3115816798545, Loss on the last accepted step: 5957.889047455692, Step size: 0.00390625\n",
            "Loss on this step: 4515.065009751052, Loss on the last accepted step: 4662.3115816798545, Step size: 0.00390625\n",
            "Loss on this step: 4418.413802576902, Loss on the last accepted step: 4515.065009751052, Step size: 0.00390625\n",
            "Loss on this step: 4389.384110953944, Loss on the last accepted step: 4418.413802576902, Step size: 0.00390625\n",
            "Loss on this step: 4377.098985190866, Loss on the last accepted step: 4389.384110953944, Step size: 0.00390625\n",
            "Loss on this step: 4373.926044373106, Loss on the last accepted step: 4377.098985190866, Step size: 0.00390625\n",
            "Loss on this step: 4371.779481785483, Loss on the last accepted step: 4373.926044373106, Step size: 0.013671875\n",
            "Loss on this step: 4366.492515493317, Loss on the last accepted step: 4371.779481785483, Step size: 0.0478515625\n",
            "Loss on this step: 4360.161373732613, Loss on the last accepted step: 4366.492515493317, Step size: 0.0478515625\n",
            "Loss on this step: 4362.033781606105, Loss on the last accepted step: 4360.161373732613, Step size: 0.011962890625\n",
            "Loss on this step: 4359.67098472254, Loss on the last accepted step: 4360.161373732613, Step size: 0.011962890625\n",
            "Loss on this step: 4359.52081671587, Loss on the last accepted step: 4359.67098472254, Step size: 0.011962890625\n",
            "Loss on this step: 4359.383365221588, Loss on the last accepted step: 4359.52081671587, Step size: 0.011962890625\n",
            "Loss on this step: 4359.340744025679, Loss on the last accepted step: 4359.383365221588, Step size: 0.011962890625\n",
            "Loss on this step: 4359.308053058353, Loss on the last accepted step: 4359.340744025679, Step size: 0.011962890625\n",
            "Loss on this step: 4359.2915105652755, Loss on the last accepted step: 4359.308053058353, Step size: 0.011962890625\n",
            "Loss on this step: 4359.276723927937, Loss on the last accepted step: 4359.2915105652755, Step size: 0.011962890625\n",
            "Loss on this step: 4359.267217623234, Loss on the last accepted step: 4359.276723927937, Step size: 0.011962890625\n",
            "Loss on this step: 4359.259123856867, Loss on the last accepted step: 4359.267217623234, Step size: 0.011962890625\n",
            "Loss on this step: 4359.252466426083, Loss on the last accepted step: 4359.259123856867, Step size: 0.011962890625\n",
            "Loss on this step: 4359.246705654759, Loss on the last accepted step: 4359.252466426083, Step size: 0.011962890625\n",
            "Loss on this step: 4359.242234524502, Loss on the last accepted step: 4359.246705654759, Step size: 0.011962890625\n",
            "Loss on this step: 4359.238538729163, Loss on the last accepted step: 4359.242234524502, Step size: 0.011962890625\n",
            "Loss on this step: 4359.235563628431, Loss on the last accepted step: 4359.238538729163, Step size: 0.011962890625\n",
            "Loss on this step: 4359.233015296204, Loss on the last accepted step: 4359.235563628431, Step size: 0.011962890625\n",
            "Loss on this step: 4359.2309310802575, Loss on the last accepted step: 4359.233015296204, Step size: 0.011962890625\n",
            "Loss on this step: 4359.229157559059, Loss on the last accepted step: 4359.2309310802575, Step size: 0.011962890625\n",
            "Loss on this step: 4359.227697662638, Loss on the last accepted step: 4359.229157559059, Step size: 0.011962890625\n",
            "Loss on this step: 4359.226439530054, Loss on the last accepted step: 4359.227697662638, Step size: 0.011962890625\n",
            "Loss on this step: 4359.225390392276, Loss on the last accepted step: 4359.226439530054, Step size: 0.011962890625\n",
            "Loss on this step: 4359.224480539513, Loss on the last accepted step: 4359.225390392276, Step size: 0.011962890625\n",
            "Loss on this step: 4359.223697755336, Loss on the last accepted step: 4359.224480539513, Step size: 0.011962890625\n",
            "Loss on this step: 4359.223017311219, Loss on the last accepted step: 4359.223697755336, Step size: 0.011962890625\n",
            "Loss on this step: 4359.2224251944235, Loss on the last accepted step: 4359.223017311219, Step size: 0.011962890625\n",
            "Loss on this step: 4359.221909899941, Loss on the last accepted step: 4359.2224251944235, Step size: 0.011962890625\n",
            "Loss on this step: 4359.221461800153, Loss on the last accepted step: 4359.221909899941, Step size: 0.011962890625\n",
            "Loss on this step: 4359.221073190444, Loss on the last accepted step: 4359.221461800153, Step size: 0.011962890625\n",
            "Loss on this step: 4359.220738093535, Loss on the last accepted step: 4359.221073190444, Step size: 0.011962890625\n",
            "Loss on this step: 4359.2204528006605, Loss on the last accepted step: 4359.220738093535, Step size: 0.011962890625\n",
            "Loss on this step: 4359.220216767542, Loss on the last accepted step: 4359.2204528006605, Step size: 0.011962890625\n",
            "Loss on this step: 4359.220034326255, Loss on the last accepted step: 4359.220216767542, Step size: 0.0418701171875\n",
            "Loss on this step: 4359.219642556484, Loss on the last accepted step: 4359.220034326255, Step size: 0.14654541015625\n",
            "Loss on this step: 4359.21927688961, Loss on the last accepted step: 4359.219642556484, Step size: 0.512908935546875\n",
            "Loss on this step: 4359.218748627567, Loss on the last accepted step: 4359.21927688961, Step size: 0.512908935546875\n",
            "Loss on this step: 4359.233074478111, Loss on the last accepted step: 4359.218748627567, Step size: 0.12822723388671875\n",
            "Loss on this step: 4359.218710219622, Loss on the last accepted step: 4359.218748627567, Step size: 0.12822723388671875\n",
            "Loss on this step: 4359.224276517987, Loss on the last accepted step: 4359.218710219622, Step size: 0.03205680847167969\n",
            "Loss on this step: 4359.218475291207, Loss on the last accepted step: 4359.218710219622, Step size: 0.03205680847167969\n",
            "Loss on this step: 4359.21834069685, Loss on the last accepted step: 4359.218475291207, Step size: 0.03205680847167969\n",
            "Loss on this step: 4359.218265088024, Loss on the last accepted step: 4359.21834069685, Step size: 0.03205680847167969\n",
            "Loss on this step: 4359.218254357514, Loss on the last accepted step: 4359.218265088024, Step size: 0.03205680847167969\n",
            "Loss on this step: 4359.218233036163, Loss on the last accepted step: 4359.218254357514, Step size: 0.03205680847167969\n",
            "Loss on this step: 4359.218231633901, Loss on the last accepted step: 4359.218233036163, Step size: 0.03205680847167969\n",
            "Loss on this step: 4359.218231000706, Loss on the last accepted step: 4359.218231633901, Step size: 0.03205680847167969\n",
            "Loss on this step: 4359.2182308188285, Loss on the last accepted step: 4359.218231000706, Step size: 0.1121988296508789\n",
            "Loss on this step: 4359.218230376182, Loss on the last accepted step: 4359.2182308188285, Step size: 0.39269590377807617\n",
            "Loss on this step: 4359.218229297612, Loss on the last accepted step: 4359.218230376182, Step size: 0.39269590377807617\n",
            "Loss on this step: 4359.218229256631, Loss on the last accepted step: 4359.218229297612, Step size: 0.39269590377807617\n",
            "Loss on this step: 4359.218229275025, Loss on the last accepted step: 4359.218229256631, Step size: 0.09817397594451904\n",
            "Loss on this step: 4359.218229243382, Loss on the last accepted step: 4359.218229256631, Step size: 0.09817397594451904\n",
            "Loss on this step: 4359.2182292405305, Loss on the last accepted step: 4359.218229243382, Step size: 0.09817397594451904\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-14 00:29:54,785 INFO jaxlogit.mixed_logit: Optimization finished, success = True, final loglike = -4359.22, final gradient max = 7.47e-05, norm = 6.75e-04.\n",
            "2025-07-14 00:29:54,786 INFO jaxlogit.mixed_logit: Calculating gradient of individual log-likelihood contributions\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss on this step: 4359.218229239384, Loss on the last accepted step: 4359.2182292405305, Step size: 0.09817397594451904\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-14 00:29:58,379 INFO jaxlogit.mixed_logit: Calculating H_inv\n",
            "2025-07-14 00:30:02,320 INFO jaxlogit._choice_model: Post fit processing\n",
            "2025-07-14 00:30:03,519 INFO jaxlogit._choice_model: Optimization terminated successfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Message: \n",
            "    Iterations: 60\n",
            "    Function evaluations: 68\n",
            "Estimation time= 29.0 seconds\n",
            "---------------------------------------------------------------------------\n",
            "Coefficient              Estimate      Std.Err.         z-val         P>|z|\n",
            "---------------------------------------------------------------------------\n",
            "ASC_CAR                 0.2831110     0.0560480     5.0512276       4.5e-07 ***\n",
            "ASC_TRAIN              -0.5722733     0.0794778    -7.2004140      6.65e-13 ***\n",
            "CO                     -1.6601666     0.0778870   -21.3150710      1.26e-97 ***\n",
            "TT                     -3.2289976     0.1749805   -18.4534737      3.16e-74 ***\n",
            "sd.TT                   3.6221628     0.1728452    20.9561137      1.58e-94 ***\n",
            "---------------------------------------------------------------------------\n",
            "Significance:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
            "\n",
            "Log-Likelihood= -4359.218\n",
            "AIC= 8728.436\n",
            "BIC= 8762.536\n"
          ]
        }
      ],
      "source": [
        "varnames=['ASC_CAR', 'ASC_TRAIN', 'CO', 'TT']\n",
        "model = MixedLogit()\n",
        "\n",
        "config = ConfigData(\n",
        "    avail=df['AV'],\n",
        "    panels=df[\"ID\"],\n",
        "    n_draws=1500,\n",
        ")\n",
        "\n",
        "res = model.fit(\n",
        "    X=df[varnames],\n",
        "    y=df['CHOICE'],\n",
        "    varnames=varnames,\n",
        "    alts=df['alt'],\n",
        "    ids=df['custom_id'],\n",
        "    randvars={'TT': 'n'},\n",
        "    config=config\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwsZACbNgJwd"
      },
      "source": [
        "The negative signs for the cost and time coefficients suggest that decision makers experience a general disutility with alternatives that have higher waiting times and costs, which conforms to the underlying decision making theory. Note that these estimates are highly consistent with those returned by Biogeme (https://biogeme.epfl.ch/examples/swissmetro/05normalMixtureIntegral.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt6rAYtH3Djj"
      },
      "source": [
        "## Electricity Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k8_iTHPn7l9"
      },
      "source": [
        "The electricity dataset contains 4,308 choices among four electricity suppliers based on the attributes of the offered plans, which include prices(pf), contract lengths(cl), time of day rates (tod), seasonal rates(seas), as well as attributes of the suppliers, which include whether the supplier is local (loc) and well-known (wk). The data was collected through a survey where 12 different choice situations were presented to each participant. The multiple responses per participants were organized into panels. Given that some participants answered less than 12 of the choice situations, some panels are unbalanced, which `xlogit` is able to handle. [Revelt and Train (1999)](https://escholarship.org/content/qt1900p96t/qt1900p96t.pdf) provide a detailed description of this dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOjSrftv3Gtm"
      },
      "source": [
        "### Read data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymoa7_h4oZo_"
      },
      "source": [
        "The dataset is already in long format so no reshaping is necessary, it can be used directly in xlogit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "fLgHickp3IJw",
        "outputId": "d5873903-e4b9-4278-cc97-a7f96c016b2a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>choice</th>\n",
              "      <th>id</th>\n",
              "      <th>alt</th>\n",
              "      <th>pf</th>\n",
              "      <th>cl</th>\n",
              "      <th>loc</th>\n",
              "      <th>wk</th>\n",
              "      <th>tod</th>\n",
              "      <th>seas</th>\n",
              "      <th>chid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17227</th>\n",
              "      <td>0</td>\n",
              "      <td>361</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17228</th>\n",
              "      <td>1</td>\n",
              "      <td>361</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17229</th>\n",
              "      <td>0</td>\n",
              "      <td>361</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17230</th>\n",
              "      <td>0</td>\n",
              "      <td>361</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17231</th>\n",
              "      <td>0</td>\n",
              "      <td>361</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4308</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17232 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       choice   id  alt  pf  cl  loc  wk  tod  seas  chid\n",
              "0           0    1    1   7   5    0   1    0     0     1\n",
              "1           0    1    2   9   1    1   0    0     0     1\n",
              "2           0    1    3   0   0    0   0    0     1     1\n",
              "3           1    1    4   0   5    0   1    1     0     1\n",
              "4           0    1    1   7   0    0   1    0     0     2\n",
              "...       ...  ...  ...  ..  ..  ...  ..  ...   ...   ...\n",
              "17227       0  361    4   0   1    1   0    0     1  4307\n",
              "17228       1  361    1   9   0    0   1    0     0  4308\n",
              "17229       0  361    2   7   0    0   0    0     0  4308\n",
              "17230       0  361    3   0   1    0   1    0     1  4308\n",
              "17231       0  361    4   0   5    1   0    1     0  4308\n",
              "\n",
              "[17232 rows x 10 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"https://raw.github.com/arteagac/xlogit/master/examples/data/electricity_long.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFUpTIpU3-Oi"
      },
      "source": [
        "### Fit the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p7isMbqoIYz"
      },
      "source": [
        "Note that the parameter `panels` was included in the `fit` function in order to take into account panel structure of this dataset during estimation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-14 00:40:23,467 INFO jaxlogit.mixed_logit: Starting data preparation, including generation of 600 random draws for each random variable and observation.\n",
            "2025-07-14 00:40:23,817 INFO jaxlogit.mixed_logit: Data contains 361 panels, using segment_sum for panel-wise log-likelihood.\n",
            "2025-07-14 00:40:23,817 INFO jaxlogit.mixed_logit: Shape of draws: (4308, 6, 600), number of draws: 600\n",
            "2025-07-14 00:40:23,818 INFO jaxlogit.mixed_logit: Shape of Xdf: (4308, 3, 0), shape of Xdr: (4308, 3, 6)\n",
            "2025-07-14 00:40:23,819 INFO jaxlogit.mixed_logit: Compiling log-likelihood function.\n",
            "2025-07-14 00:40:23,888 INFO jaxlogit.mixed_logit: Compilation finished, init neg_loglike = 5413.77, params= [(np.str_('pf'), Array(0.1, dtype=float64)), (np.str_('cl'), Array(0.1, dtype=float64)), (np.str_('loc'), Array(0.1, dtype=float64)), (np.str_('wk'), Array(0.1, dtype=float64)), (np.str_('tod'), Array(0.1, dtype=float64)), (np.str_('seas'), Array(0.1, dtype=float64)), (np.str_('sd.pf'), Array(0.1, dtype=float64)), (np.str_('sd.cl'), Array(0.1, dtype=float64)), (np.str_('sd.loc'), Array(0.1, dtype=float64)), (np.str_('sd.wk'), Array(0.1, dtype=float64)), (np.str_('sd.tod'), Array(0.1, dtype=float64)), (np.str_('sd.seas'), Array(0.1, dtype=float64))]\n",
            "2025-07-14 00:40:23,889 INFO jaxlogit._optimize: Running minimization with method trust-region\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss on this step: 5413.773124872253, Loss on the last accepted step: 0.0, Step size: 1.0\n",
            "Loss on this step: 246611.46044373326, Loss on the last accepted step: 5413.773124872253, Step size: 0.25\n",
            "Loss on this step: 237649.62180272705, Loss on the last accepted step: 5413.773124872253, Step size: 0.0625\n",
            "Loss on this step: 134257.0137458888, Loss on the last accepted step: 5413.773124872253, Step size: 0.015625\n",
            "Loss on this step: 34162.8291987072, Loss on the last accepted step: 5413.773124872253, Step size: 0.00390625\n",
            "Loss on this step: 5359.06864908849, Loss on the last accepted step: 5413.773124872253, Step size: 0.00390625\n",
            "Loss on this step: 15502.842318962963, Loss on the last accepted step: 5359.06864908849, Step size: 0.0009765625\n",
            "Loss on this step: 4871.405959053192, Loss on the last accepted step: 5359.06864908849, Step size: 0.0009765625\n",
            "Loss on this step: 4832.977340051752, Loss on the last accepted step: 4871.405959053192, Step size: 0.0009765625\n",
            "Loss on this step: 4703.348617009259, Loss on the last accepted step: 4832.977340051752, Step size: 0.0009765625\n",
            "Loss on this step: 4620.68272829984, Loss on the last accepted step: 4703.348617009259, Step size: 0.0009765625\n",
            "Loss on this step: 4587.261906554008, Loss on the last accepted step: 4620.68272829984, Step size: 0.0009765625\n",
            "Loss on this step: 4555.396824121282, Loss on the last accepted step: 4587.261906554008, Step size: 0.0009765625\n",
            "Loss on this step: 4534.777908599345, Loss on the last accepted step: 4555.396824121282, Step size: 0.0009765625\n",
            "Loss on this step: 4518.7324569957755, Loss on the last accepted step: 4534.777908599345, Step size: 0.0009765625\n",
            "Loss on this step: 4503.156845658847, Loss on the last accepted step: 4518.7324569957755, Step size: 0.0009765625\n",
            "Loss on this step: 4490.2676887957505, Loss on the last accepted step: 4503.156845658847, Step size: 0.0009765625\n",
            "Loss on this step: 4478.044889994004, Loss on the last accepted step: 4490.2676887957505, Step size: 0.0009765625\n",
            "Loss on this step: 4467.064297757031, Loss on the last accepted step: 4478.044889994004, Step size: 0.0009765625\n",
            "Loss on this step: 4456.439748102932, Loss on the last accepted step: 4467.064297757031, Step size: 0.0009765625\n",
            "Loss on this step: 4446.8540399875765, Loss on the last accepted step: 4456.439748102932, Step size: 0.0009765625\n",
            "Loss on this step: 4437.5854782105625, Loss on the last accepted step: 4446.8540399875765, Step size: 0.0009765625\n",
            "Loss on this step: 4428.941413381126, Loss on the last accepted step: 4437.5854782105625, Step size: 0.0009765625\n",
            "Loss on this step: 4420.5400635723945, Loss on the last accepted step: 4428.941413381126, Step size: 0.0009765625\n",
            "Loss on this step: 4412.715193624076, Loss on the last accepted step: 4420.5400635723945, Step size: 0.0009765625\n",
            "Loss on this step: 4405.0645307609475, Loss on the last accepted step: 4412.715193624076, Step size: 0.0009765625\n",
            "Loss on this step: 4397.819263948091, Loss on the last accepted step: 4405.0645307609475, Step size: 0.0009765625\n",
            "Loss on this step: 4390.739079677354, Loss on the last accepted step: 4397.819263948091, Step size: 0.0009765625\n",
            "Loss on this step: 4384.025763458956, Loss on the last accepted step: 4390.739079677354, Step size: 0.0009765625\n",
            "Loss on this step: 4377.437598440404, Loss on the last accepted step: 4384.025763458956, Step size: 0.0009765625\n",
            "Loss on this step: 4371.141126410086, Loss on the last accepted step: 4377.437598440404, Step size: 0.0009765625\n",
            "Loss on this step: 4364.967574053235, Loss on the last accepted step: 4371.141126410086, Step size: 0.0009765625\n",
            "Loss on this step: 4359.052665393784, Loss on the last accepted step: 4364.967574053235, Step size: 0.0009765625\n",
            "Loss on this step: 4353.241329790852, Loss on the last accepted step: 4359.052665393784, Step size: 0.0009765625\n",
            "Loss on this step: 4347.649964867589, Loss on the last accepted step: 4353.241329790852, Step size: 0.0009765625\n",
            "Loss on this step: 4342.158516721553, Loss on the last accepted step: 4347.649964867589, Step size: 0.0009765625\n",
            "Loss on this step: 4336.861767098882, Loss on the last accepted step: 4342.158516721553, Step size: 0.0009765625\n",
            "Loss on this step: 4331.655178744376, Loss on the last accepted step: 4336.861767098882, Step size: 0.0009765625\n",
            "Loss on this step: 4326.619650700131, Loss on the last accepted step: 4331.655178744376, Step size: 0.00341796875\n",
            "Loss on this step: 4310.5636110217765, Loss on the last accepted step: 4326.619650700131, Step size: 0.00341796875\n",
            "Loss on this step: 4296.143475999875, Loss on the last accepted step: 4310.5636110217765, Step size: 0.00341796875\n",
            "Loss on this step: 4272.1499119105365, Loss on the last accepted step: 4296.143475999875, Step size: 0.00341796875\n",
            "Loss on this step: 4275.554869643499, Loss on the last accepted step: 4272.1499119105365, Step size: 0.0008544921875\n",
            "Loss on this step: 4268.299069099116, Loss on the last accepted step: 4272.1499119105365, Step size: 0.0008544921875\n",
            "Loss on this step: 4265.067022681428, Loss on the last accepted step: 4268.299069099116, Step size: 0.0008544921875\n",
            "Loss on this step: 4262.497798939609, Loss on the last accepted step: 4265.067022681428, Step size: 0.0008544921875\n",
            "Loss on this step: 4259.9437011812215, Loss on the last accepted step: 4262.497798939609, Step size: 0.00299072265625\n",
            "Loss on this step: 4250.287328052741, Loss on the last accepted step: 4259.9437011812215, Step size: 0.00299072265625\n",
            "Loss on this step: 4242.009516194581, Loss on the last accepted step: 4250.287328052741, Step size: 0.00299072265625\n",
            "Loss on this step: 4234.672950818367, Loss on the last accepted step: 4242.009516194581, Step size: 0.00299072265625\n",
            "Loss on this step: 4228.244551219933, Loss on the last accepted step: 4234.672950818367, Step size: 0.00299072265625\n",
            "Loss on this step: 4221.897065287043, Loss on the last accepted step: 4228.244551219933, Step size: 0.00299072265625\n",
            "Loss on this step: 4216.093326917631, Loss on the last accepted step: 4221.897065287043, Step size: 0.00299072265625\n",
            "Loss on this step: 4210.446804248867, Loss on the last accepted step: 4216.093326917631, Step size: 0.00299072265625\n",
            "Loss on this step: 4205.267341222326, Loss on the last accepted step: 4210.446804248867, Step size: 0.00299072265625\n",
            "Loss on this step: 4200.1762279915765, Loss on the last accepted step: 4205.267341222326, Step size: 0.00299072265625\n",
            "Loss on this step: 4195.4460810142045, Loss on the last accepted step: 4200.1762279915765, Step size: 0.010467529296875\n",
            "Loss on this step: 4180.2874776697745, Loss on the last accepted step: 4195.4460810142045, Step size: 0.010467529296875\n",
            "Loss on this step: 4168.4233683882685, Loss on the last accepted step: 4180.2874776697745, Step size: 0.010467529296875\n",
            "Loss on this step: 4150.615199709961, Loss on the last accepted step: 4168.4233683882685, Step size: 0.010467529296875\n",
            "Loss on this step: 4145.9785148588135, Loss on the last accepted step: 4150.615199709961, Step size: 0.010467529296875\n",
            "Loss on this step: 4116.220868654043, Loss on the last accepted step: 4145.9785148588135, Step size: 0.010467529296875\n",
            "Loss on this step: 4107.9539177357865, Loss on the last accepted step: 4116.220868654043, Step size: 0.010467529296875\n",
            "Loss on this step: 4098.28868127311, Loss on the last accepted step: 4107.9539177357865, Step size: 0.010467529296875\n",
            "Loss on this step: 4089.802882933807, Loss on the last accepted step: 4098.28868127311, Step size: 0.010467529296875\n",
            "Loss on this step: 4081.9428025715733, Loss on the last accepted step: 4089.802882933807, Step size: 0.010467529296875\n",
            "Loss on this step: 4075.618947578087, Loss on the last accepted step: 4081.9428025715733, Step size: 0.010467529296875\n",
            "Loss on this step: 4068.263214485156, Loss on the last accepted step: 4075.618947578087, Step size: 0.010467529296875\n",
            "Loss on this step: 4061.8589259503706, Loss on the last accepted step: 4068.263214485156, Step size: 0.010467529296875\n",
            "Loss on this step: 4054.2670415225875, Loss on the last accepted step: 4061.8589259503706, Step size: 0.010467529296875\n",
            "Loss on this step: 4047.8495625082664, Loss on the last accepted step: 4054.2670415225875, Step size: 0.010467529296875\n",
            "Loss on this step: 4042.8830544317343, Loss on the last accepted step: 4047.8495625082664, Step size: 0.010467529296875\n",
            "Loss on this step: 4037.9679384139977, Loss on the last accepted step: 4042.8830544317343, Step size: 0.010467529296875\n",
            "Loss on this step: 4033.7855510046065, Loss on the last accepted step: 4037.9679384139977, Step size: 0.010467529296875\n",
            "Loss on this step: 4029.890821814091, Loss on the last accepted step: 4033.7855510046065, Step size: 0.010467529296875\n",
            "Loss on this step: 4025.819715318226, Loss on the last accepted step: 4029.890821814091, Step size: 0.010467529296875\n",
            "Loss on this step: 4022.453209030839, Loss on the last accepted step: 4025.819715318226, Step size: 0.010467529296875\n",
            "Loss on this step: 4018.3244171735555, Loss on the last accepted step: 4022.453209030839, Step size: 0.010467529296875\n",
            "Loss on this step: 4014.749992885716, Loss on the last accepted step: 4018.3244171735555, Step size: 0.010467529296875\n",
            "Loss on this step: 4010.3969765075385, Loss on the last accepted step: 4014.749992885716, Step size: 0.010467529296875\n",
            "Loss on this step: 4007.0279095554806, Loss on the last accepted step: 4010.3969765075385, Step size: 0.010467529296875\n",
            "Loss on this step: 4003.61723083291, Loss on the last accepted step: 4007.0279095554806, Step size: 0.010467529296875\n",
            "Loss on this step: 4000.4043372119486, Loss on the last accepted step: 4003.61723083291, Step size: 0.010467529296875\n",
            "Loss on this step: 3997.9839950386317, Loss on the last accepted step: 4000.4043372119486, Step size: 0.010467529296875\n",
            "Loss on this step: 3994.954925945188, Loss on the last accepted step: 3997.9839950386317, Step size: 0.010467529296875\n",
            "Loss on this step: 3992.848810140067, Loss on the last accepted step: 3994.954925945188, Step size: 0.010467529296875\n",
            "Loss on this step: 3990.071307104807, Loss on the last accepted step: 3992.848810140067, Step size: 0.010467529296875\n",
            "Loss on this step: 3988.0088337468337, Loss on the last accepted step: 3990.071307104807, Step size: 0.010467529296875\n",
            "Loss on this step: 3985.3933376863833, Loss on the last accepted step: 3988.0088337468337, Step size: 0.0366363525390625\n",
            "Loss on this step: 3978.8416301493894, Loss on the last accepted step: 3985.3933376863833, Step size: 0.0366363525390625\n",
            "Loss on this step: 3971.0047232332777, Loss on the last accepted step: 3978.8416301493894, Step size: 0.0366363525390625\n",
            "Loss on this step: 3995.5386127503807, Loss on the last accepted step: 3971.0047232332777, Step size: 0.009159088134765625\n",
            "Loss on this step: 3970.161365052137, Loss on the last accepted step: 3971.0047232332777, Step size: 0.009159088134765625\n",
            "Loss on this step: 3967.219225387362, Loss on the last accepted step: 3970.161365052137, Step size: 0.009159088134765625\n",
            "Loss on this step: 3965.1105966562445, Loss on the last accepted step: 3967.219225387362, Step size: 0.009159088134765625\n",
            "Loss on this step: 3963.30154198859, Loss on the last accepted step: 3965.1105966562445, Step size: 0.009159088134765625\n",
            "Loss on this step: 3961.992381851366, Loss on the last accepted step: 3963.30154198859, Step size: 0.03205680847167969\n",
            "Loss on this step: 3957.4537124620792, Loss on the last accepted step: 3961.992381851366, Step size: 0.1121988296508789\n",
            "Loss on this step: 3945.3282738106827, Loss on the last accepted step: 3957.4537124620792, Step size: 0.1121988296508789\n",
            "Loss on this step: 3951.6153520592984, Loss on the last accepted step: 3945.3282738106827, Step size: 0.028049707412719727\n",
            "Loss on this step: 3931.6730101708667, Loss on the last accepted step: 3945.3282738106827, Step size: 0.028049707412719727\n",
            "Loss on this step: 3928.89002814149, Loss on the last accepted step: 3931.6730101708667, Step size: 0.028049707412719727\n",
            "Loss on this step: 3925.5014048860244, Loss on the last accepted step: 3928.89002814149, Step size: 0.028049707412719727\n",
            "Loss on this step: 3923.3676432759476, Loss on the last accepted step: 3925.5014048860244, Step size: 0.028049707412719727\n",
            "Loss on this step: 3920.7415786045785, Loss on the last accepted step: 3923.3676432759476, Step size: 0.028049707412719727\n",
            "Loss on this step: 3919.3492585426943, Loss on the last accepted step: 3920.7415786045785, Step size: 0.028049707412719727\n",
            "Loss on this step: 3916.5963484127287, Loss on the last accepted step: 3919.3492585426943, Step size: 0.028049707412719727\n",
            "Loss on this step: 3915.369244639701, Loss on the last accepted step: 3916.5963484127287, Step size: 0.028049707412719727\n",
            "Loss on this step: 3913.563070953317, Loss on the last accepted step: 3915.369244639701, Step size: 0.028049707412719727\n",
            "Loss on this step: 3912.3553192371596, Loss on the last accepted step: 3913.563070953317, Step size: 0.09817397594451904\n",
            "Loss on this step: 3908.2501733674994, Loss on the last accepted step: 3912.3553192371596, Step size: 0.09817397594451904\n",
            "Loss on this step: 3904.551076054738, Loss on the last accepted step: 3908.2501733674994, Step size: 0.09817397594451904\n",
            "Loss on this step: 3899.99348807683, Loss on the last accepted step: 3904.551076054738, Step size: 0.09817397594451904\n",
            "Loss on this step: 3893.821926890298, Loss on the last accepted step: 3899.99348807683, Step size: 0.09817397594451904\n",
            "Loss on this step: 3892.330189874096, Loss on the last accepted step: 3893.821926890298, Step size: 0.09817397594451904\n",
            "Loss on this step: 3891.281994569046, Loss on the last accepted step: 3892.330189874096, Step size: 0.09817397594451904\n",
            "Loss on this step: 3890.422828484102, Loss on the last accepted step: 3891.281994569046, Step size: 0.09817397594451904\n",
            "Loss on this step: 3889.957889349677, Loss on the last accepted step: 3890.422828484102, Step size: 0.09817397594451904\n",
            "Loss on this step: 3889.551000343231, Loss on the last accepted step: 3889.957889349677, Step size: 0.09817397594451904\n",
            "Loss on this step: 3889.263808758708, Loss on the last accepted step: 3889.551000343231, Step size: 0.09817397594451904\n",
            "Loss on this step: 3889.033631248075, Loss on the last accepted step: 3889.263808758708, Step size: 0.09817397594451904\n",
            "Loss on this step: 3888.928823385868, Loss on the last accepted step: 3889.033631248075, Step size: 0.09817397594451904\n",
            "Loss on this step: 3888.7555678059043, Loss on the last accepted step: 3888.928823385868, Step size: 0.09817397594451904\n",
            "Loss on this step: 3888.693976332315, Loss on the last accepted step: 3888.7555678059043, Step size: 0.09817397594451904\n",
            "Loss on this step: 3888.6468918643495, Loss on the last accepted step: 3888.693976332315, Step size: 0.09817397594451904\n",
            "Loss on this step: 3888.6025073808382, Loss on the last accepted step: 3888.6468918643495, Step size: 0.09817397594451904\n",
            "Loss on this step: 3888.5703114678236, Loss on the last accepted step: 3888.6025073808382, Step size: 0.09817397594451904\n",
            "Loss on this step: 3888.5429226477127, Loss on the last accepted step: 3888.5703114678236, Step size: 0.09817397594451904\n",
            "Loss on this step: 3888.526768055173, Loss on the last accepted step: 3888.5429226477127, Step size: 0.09817397594451904\n",
            "Loss on this step: 3888.5141778014213, Loss on the last accepted step: 3888.526768055173, Step size: 0.09817397594451904\n",
            "Loss on this step: 3888.5044790274983, Loss on the last accepted step: 3888.5141778014213, Step size: 0.34360891580581665\n",
            "Loss on this step: 3888.482087220388, Loss on the last accepted step: 3888.5044790274983, Step size: 1.2026312053203583\n",
            "Loss on this step: 3888.4655780250473, Loss on the last accepted step: 3888.482087220388, Step size: 1.2026312053203583\n",
            "Loss on this step: 3888.465664048932, Loss on the last accepted step: 3888.4655780250473, Step size: 0.30065780133008957\n",
            "Loss on this step: 3888.465180388699, Loss on the last accepted step: 3888.4655780250473, Step size: 0.30065780133008957\n",
            "Loss on this step: 3888.4651038552993, Loss on the last accepted step: 3888.465180388699, Step size: 0.30065780133008957\n",
            "Loss on this step: 3888.465085100385, Loss on the last accepted step: 3888.4651038552993, Step size: 0.30065780133008957\n",
            "Loss on this step: 3888.465076612722, Loss on the last accepted step: 3888.465085100385, Step size: 0.30065780133008957\n",
            "Loss on this step: 3888.4650729850955, Loss on the last accepted step: 3888.465076612722, Step size: 1.0523023046553135\n",
            "Loss on this step: 3888.4650698083597, Loss on the last accepted step: 3888.4650729850955, Step size: 3.683058066293597\n",
            "Loss on this step: 3888.4650698042415, Loss on the last accepted step: 3888.4650698083597, Step size: 3.683058066293597\n",
            "Loss on this step: 3888.4650698020546, Loss on the last accepted step: 3888.4650698042415, Step size: 3.683058066293597\n",
            "Loss on this step: 3888.4650698020487, Loss on the last accepted step: 3888.4650698020546, Step size: 3.683058066293597\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-14 00:40:43,541 INFO jaxlogit.mixed_logit: Optimization finished, success = True, final loglike = -3888.47, final gradient max = 1.22e-05, norm = 2.12e-05.\n",
            "2025-07-14 00:40:43,543 INFO jaxlogit.mixed_logit: Calculating gradient of individual log-likelihood contributions\n",
            "2025-07-14 00:40:45,909 INFO jaxlogit.mixed_logit: Calculating H_inv\n",
            "2025-07-14 00:40:49,367 INFO jaxlogit._choice_model: Post fit processing\n",
            "2025-07-14 00:40:50,018 INFO jaxlogit._choice_model: Optimization terminated successfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Message: \n",
            "    Iterations: 134\n",
            "    Function evaluations: 143\n",
            "Estimation time= 26.5 seconds\n",
            "---------------------------------------------------------------------------\n",
            "Coefficient              Estimate      Std.Err.         z-val         P>|z|\n",
            "---------------------------------------------------------------------------\n",
            "pf                     -0.9972118     0.0378219   -26.3660068     3.71e-142 ***\n",
            "cl                     -0.2196750     0.0255168    -8.6090393      1.02e-17 ***\n",
            "loc                     2.2902246     0.1263057    18.1323913      7.19e-71 ***\n",
            "wk                      1.6943092     0.0961498    17.6215603      3.63e-67 ***\n",
            "tod                    -9.6752577     0.3350901   -28.8736009     9.73e-168 ***\n",
            "seas                   -9.6962299     0.3246517   -29.8665593     2.65e-178 ***\n",
            "sd.pf                  -1.3984357     0.0964657   -14.4967194      1.56e-46 ***\n",
            "sd.cl                  -0.6749717     0.0754671    -8.9439189      5.47e-19 ***\n",
            "sd.loc                  1.6001547     0.1419789    11.2703682      4.67e-29 ***\n",
            "sd.wk                   0.8837436     0.1333533     6.6270841      3.85e-11 ***\n",
            "sd.tod                  2.1673159     0.1936626    11.1911952      1.12e-28 ***\n",
            "sd.seas                 1.2297691     0.2117376     5.8079872      6.78e-09 ***\n",
            "---------------------------------------------------------------------------\n",
            "Significance:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
            "\n",
            "Log-Likelihood= -3888.465\n",
            "AIC= 7800.930\n",
            "BIC= 7877.349\n"
          ]
        }
      ],
      "source": [
        "varnames = ['pf', 'cl', 'loc', 'wk', 'tod', 'seas']\n",
        "model = MixedLogit()\n",
        "\n",
        "config = ConfigData(\n",
        "    n_draws=600,\n",
        "    panels=df['id'],\n",
        ")\n",
        "\n",
        "res = model.fit(\n",
        "    X=df[varnames],\n",
        "    y=df['choice'],\n",
        "    varnames=varnames,\n",
        "    ids=df['chid'],\n",
        "    alts=df['alt'],\n",
        "    randvars={'pf': 'n', 'cl': 'n', 'loc': 'n', 'wk': 'n', 'tod': 'n', 'seas': 'n'},\n",
        "    config=config\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-14 00:40:50,037 INFO jaxlogit.mixed_logit: Starting data preparation, including generation of 600 random draws for each random variable and observation.\n",
            "2025-07-14 00:40:50,413 INFO jaxlogit.mixed_logit: Data contains 361 panels, using segment_sum for panel-wise log-likelihood.\n",
            "2025-07-14 00:40:50,413 INFO jaxlogit.mixed_logit: Shape of draws: (4308, 6, 600), number of draws: 600\n",
            "2025-07-14 00:40:50,414 INFO jaxlogit.mixed_logit: Shape of Xdf: (4308, 3, 0), shape of Xdr: (4308, 3, 6)\n",
            "2025-07-14 00:40:50,415 INFO jaxlogit.mixed_logit: Compiling log-likelihood function.\n",
            "2025-07-14 00:40:50,624 INFO jaxlogit.mixed_logit: Compilation finished, init neg_loglike = 5620.08, params= [(np.str_('pf'), Array(0.1, dtype=float64)), (np.str_('cl'), Array(0.1, dtype=float64)), (np.str_('loc'), Array(0.1, dtype=float64)), (np.str_('wk'), Array(0.1, dtype=float64)), (np.str_('tod'), Array(0.1, dtype=float64)), (np.str_('seas'), Array(0.1, dtype=float64)), (np.str_('sd.pf'), Array(0.1, dtype=float64)), (np.str_('sd.cl'), Array(0.1, dtype=float64)), (np.str_('sd.loc'), Array(0.1, dtype=float64)), (np.str_('sd.wk'), Array(0.1, dtype=float64)), (np.str_('sd.tod'), Array(0.1, dtype=float64)), (np.str_('sd.seas'), Array(0.1, dtype=float64))]\n",
            "2025-07-14 00:40:50,626 INFO jaxlogit._optimize: Running minimization with method trust-region\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss on this step: 5620.078112126817, Loss on the last accepted step: 0.0, Step size: 1.0\n",
            "Loss on this step: 237583.03657902256, Loss on the last accepted step: 5620.078112126817, Step size: 0.25\n",
            "Loss on this step: 186550.21436822342, Loss on the last accepted step: 5620.078112126817, Step size: 0.0625\n",
            "Loss on this step: 59946.26708452902, Loss on the last accepted step: 5620.078112126817, Step size: 0.015625\n",
            "Loss on this step: 17214.20567921159, Loss on the last accepted step: 5620.078112126817, Step size: 0.00390625\n",
            "Loss on this step: 7648.936515317229, Loss on the last accepted step: 5620.078112126817, Step size: 0.0009765625\n",
            "Loss on this step: 6172.239268016281, Loss on the last accepted step: 5620.078112126817, Step size: 0.000244140625\n",
            "Loss on this step: 5539.260682475645, Loss on the last accepted step: 5620.078112126817, Step size: 0.000244140625\n",
            "Loss on this step: 5397.039549904901, Loss on the last accepted step: 5539.260682475645, Step size: 0.0008544921875\n",
            "Loss on this step: 15223.48163895927, Loss on the last accepted step: 5397.039549904901, Step size: 0.000213623046875\n",
            "Loss on this step: 6779.624460331542, Loss on the last accepted step: 5397.039549904901, Step size: 5.340576171875e-05\n",
            "Loss on this step: 5993.333985338058, Loss on the last accepted step: 5397.039549904901, Step size: 1.33514404296875e-05\n",
            "Loss on this step: 5468.311612846002, Loss on the last accepted step: 5397.039549904901, Step size: 3.337860107421875e-06\n",
            "Loss on this step: 5299.145227419269, Loss on the last accepted step: 5397.039549904901, Step size: 3.337860107421875e-06\n",
            "Loss on this step: 5296.9731927191115, Loss on the last accepted step: 5299.145227419269, Step size: 1.1682510375976562e-05\n",
            "Loss on this step: 5288.936004544356, Loss on the last accepted step: 5296.9731927191115, Step size: 4.088878631591797e-05\n",
            "Loss on this step: 5262.631537332012, Loss on the last accepted step: 5288.936004544356, Step size: 0.0001431107521057129\n",
            "Loss on this step: 5177.038956906612, Loss on the last accepted step: 5262.631537332012, Step size: 0.0001431107521057129\n",
            "Loss on this step: 5119.626660373758, Loss on the last accepted step: 5177.038956906612, Step size: 0.0001431107521057129\n",
            "Loss on this step: 5067.283918004201, Loss on the last accepted step: 5119.626660373758, Step size: 0.0001431107521057129\n",
            "Loss on this step: 5041.896999336672, Loss on the last accepted step: 5067.283918004201, Step size: 0.0001431107521057129\n",
            "Loss on this step: 5009.197352077929, Loss on the last accepted step: 5041.896999336672, Step size: 0.0005008876323699951\n",
            "Loss on this step: 4940.064164516762, Loss on the last accepted step: 5009.197352077929, Step size: 0.0005008876323699951\n",
            "Loss on this step: 4874.385205002962, Loss on the last accepted step: 4940.064164516762, Step size: 0.0005008876323699951\n",
            "Loss on this step: 4835.284137183733, Loss on the last accepted step: 4874.385205002962, Step size: 0.0005008876323699951\n",
            "Loss on this step: 4790.246744697411, Loss on the last accepted step: 4835.284137183733, Step size: 0.001753106713294983\n",
            "Loss on this step: 4944.260258546018, Loss on the last accepted step: 4790.246744697411, Step size: 0.00043827667832374573\n",
            "Loss on this step: 4776.063883187993, Loss on the last accepted step: 4790.246744697411, Step size: 0.00043827667832374573\n",
            "Loss on this step: 4746.667500915887, Loss on the last accepted step: 4776.063883187993, Step size: 0.00043827667832374573\n",
            "Loss on this step: 4732.9448361811355, Loss on the last accepted step: 4746.667500915887, Step size: 0.00043827667832374573\n",
            "Loss on this step: 4717.9772948862565, Loss on the last accepted step: 4732.9448361811355, Step size: 0.00043827667832374573\n",
            "Loss on this step: 4704.025677869967, Loss on the last accepted step: 4717.9772948862565, Step size: 0.00153396837413311\n",
            "Loss on this step: 4669.83419845679, Loss on the last accepted step: 4704.025677869967, Step size: 0.00153396837413311\n",
            "Loss on this step: 4650.198135782869, Loss on the last accepted step: 4669.83419845679, Step size: 0.00153396837413311\n",
            "Loss on this step: 4555.225544444663, Loss on the last accepted step: 4650.198135782869, Step size: 0.005368889309465885\n",
            "Loss on this step: 4655.889115024613, Loss on the last accepted step: 4555.225544444663, Step size: 0.0013422223273664713\n",
            "Loss on this step: 4547.3173758874145, Loss on the last accepted step: 4555.225544444663, Step size: 0.0013422223273664713\n",
            "Loss on this step: 4508.616694169101, Loss on the last accepted step: 4547.3173758874145, Step size: 0.0013422223273664713\n",
            "Loss on this step: 4500.288159196001, Loss on the last accepted step: 4508.616694169101, Step size: 0.0013422223273664713\n",
            "Loss on this step: 4476.733832788563, Loss on the last accepted step: 4500.288159196001, Step size: 0.0013422223273664713\n",
            "Loss on this step: 4451.545573672129, Loss on the last accepted step: 4476.733832788563, Step size: 0.0013422223273664713\n",
            "Loss on this step: 4442.333050870035, Loss on the last accepted step: 4451.545573672129, Step size: 0.0013422223273664713\n",
            "Loss on this step: 4422.488807006731, Loss on the last accepted step: 4442.333050870035, Step size: 0.0013422223273664713\n",
            "Loss on this step: 4409.20608259869, Loss on the last accepted step: 4422.488807006731, Step size: 0.0013422223273664713\n",
            "Loss on this step: 4398.296512817709, Loss on the last accepted step: 4409.20608259869, Step size: 0.0013422223273664713\n",
            "Loss on this step: 4388.831698084534, Loss on the last accepted step: 4398.296512817709, Step size: 0.0013422223273664713\n",
            "Loss on this step: 4379.766415710713, Loss on the last accepted step: 4388.831698084534, Step size: 0.0013422223273664713\n",
            "Loss on this step: 4371.848775893495, Loss on the last accepted step: 4379.766415710713, Step size: 0.0013422223273664713\n",
            "Loss on this step: 4363.871852920235, Loss on the last accepted step: 4371.848775893495, Step size: 0.0013422223273664713\n",
            "Loss on this step: 4356.877954414316, Loss on the last accepted step: 4363.871852920235, Step size: 0.0013422223273664713\n",
            "Loss on this step: 4349.836254419746, Loss on the last accepted step: 4356.877954414316, Step size: 0.0013422223273664713\n",
            "Loss on this step: 4343.528685074759, Loss on the last accepted step: 4349.836254419746, Step size: 0.0046977781457826495\n",
            "Loss on this step: 4323.13960600836, Loss on the last accepted step: 4343.528685074759, Step size: 0.0046977781457826495\n",
            "Loss on this step: 4306.416695288695, Loss on the last accepted step: 4323.13960600836, Step size: 0.0046977781457826495\n",
            "Loss on this step: 4284.318200019168, Loss on the last accepted step: 4306.416695288695, Step size: 0.0046977781457826495\n",
            "Loss on this step: 4285.138272691009, Loss on the last accepted step: 4284.318200019168, Step size: 0.0011744445364456624\n",
            "Loss on this step: 4281.432015797148, Loss on the last accepted step: 4284.318200019168, Step size: 0.0011744445364456624\n",
            "Loss on this step: 4278.96132421259, Loss on the last accepted step: 4281.432015797148, Step size: 0.0011744445364456624\n",
            "Loss on this step: 4276.344676392994, Loss on the last accepted step: 4278.96132421259, Step size: 0.0011744445364456624\n",
            "Loss on this step: 4273.812963584018, Loss on the last accepted step: 4276.344676392994, Step size: 0.0011744445364456624\n",
            "Loss on this step: 4271.424654809803, Loss on the last accepted step: 4273.812963584018, Step size: 0.004110555877559818\n",
            "Loss on this step: 4262.762667358231, Loss on the last accepted step: 4271.424654809803, Step size: 0.004110555877559818\n",
            "Loss on this step: 4254.73970595982, Loss on the last accepted step: 4262.762667358231, Step size: 0.004110555877559818\n",
            "Loss on this step: 4247.995417361457, Loss on the last accepted step: 4254.73970595982, Step size: 0.004110555877559818\n",
            "Loss on this step: 4240.383313268667, Loss on the last accepted step: 4247.995417361457, Step size: 0.004110555877559818\n",
            "Loss on this step: 4234.066669813113, Loss on the last accepted step: 4240.383313268667, Step size: 0.004110555877559818\n",
            "Loss on this step: 4226.9144686519, Loss on the last accepted step: 4234.066669813113, Step size: 0.004110555877559818\n",
            "Loss on this step: 4220.537405442743, Loss on the last accepted step: 4226.9144686519, Step size: 0.004110555877559818\n",
            "Loss on this step: 4214.365501109196, Loss on the last accepted step: 4220.537405442743, Step size: 0.004110555877559818\n",
            "Loss on this step: 4208.624355571494, Loss on the last accepted step: 4214.365501109196, Step size: 0.004110555877559818\n",
            "Loss on this step: 4203.432307120458, Loss on the last accepted step: 4208.624355571494, Step size: 0.004110555877559818\n",
            "Loss on this step: 4198.1349882720115, Loss on the last accepted step: 4203.432307120458, Step size: 0.004110555877559818\n",
            "Loss on this step: 4193.695985359089, Loss on the last accepted step: 4198.1349882720115, Step size: 0.004110555877559818\n",
            "Loss on this step: 4188.980435520634, Loss on the last accepted step: 4193.695985359089, Step size: 0.004110555877559818\n",
            "Loss on this step: 4185.209923531973, Loss on the last accepted step: 4188.980435520634, Step size: 0.004110555877559818\n",
            "Loss on this step: 4181.10481443482, Loss on the last accepted step: 4185.209923531973, Step size: 0.004110555877559818\n",
            "Loss on this step: 4177.414804912595, Loss on the last accepted step: 4181.10481443482, Step size: 0.014386945571459364\n",
            "Loss on this step: 4165.5668924907295, Loss on the last accepted step: 4177.414804912595, Step size: 0.014386945571459364\n",
            "Loss on this step: 4154.9136154756225, Loss on the last accepted step: 4165.5668924907295, Step size: 0.014386945571459364\n",
            "Loss on this step: 4143.242854462477, Loss on the last accepted step: 4154.9136154756225, Step size: 0.014386945571459364\n",
            "Loss on this step: 4137.79586841269, Loss on the last accepted step: 4143.242854462477, Step size: 0.014386945571459364\n",
            "Loss on this step: 4121.991119279197, Loss on the last accepted step: 4137.79586841269, Step size: 0.014386945571459364\n",
            "Loss on this step: 4115.471022230467, Loss on the last accepted step: 4121.991119279197, Step size: 0.014386945571459364\n",
            "Loss on this step: 4104.495699450098, Loss on the last accepted step: 4115.471022230467, Step size: 0.014386945571459364\n",
            "Loss on this step: 4096.715809916957, Loss on the last accepted step: 4104.495699450098, Step size: 0.014386945571459364\n",
            "Loss on this step: 4087.893820280715, Loss on the last accepted step: 4096.715809916957, Step size: 0.014386945571459364\n",
            "Loss on this step: 4079.4956415686856, Loss on the last accepted step: 4087.893820280715, Step size: 0.014386945571459364\n",
            "Loss on this step: 4071.8217957883094, Loss on the last accepted step: 4079.4956415686856, Step size: 0.014386945571459364\n",
            "Loss on this step: 4064.087084023165, Loss on the last accepted step: 4071.8217957883094, Step size: 0.014386945571459364\n",
            "Loss on this step: 4058.819668803633, Loss on the last accepted step: 4064.087084023165, Step size: 0.014386945571459364\n",
            "Loss on this step: 4053.5127884759604, Loss on the last accepted step: 4058.819668803633, Step size: 0.014386945571459364\n",
            "Loss on this step: 4048.955094301849, Loss on the last accepted step: 4053.5127884759604, Step size: 0.014386945571459364\n",
            "Loss on this step: 4043.718438856083, Loss on the last accepted step: 4048.955094301849, Step size: 0.014386945571459364\n",
            "Loss on this step: 4039.6502528350406, Loss on the last accepted step: 4043.718438856083, Step size: 0.014386945571459364\n",
            "Loss on this step: 4035.25382488759, Loss on the last accepted step: 4039.6502528350406, Step size: 0.014386945571459364\n",
            "Loss on this step: 4031.505782852765, Loss on the last accepted step: 4035.25382488759, Step size: 0.014386945571459364\n",
            "Loss on this step: 4027.6239174068437, Loss on the last accepted step: 4031.505782852765, Step size: 0.014386945571459364\n",
            "Loss on this step: 4024.073783956449, Loss on the last accepted step: 4027.6239174068437, Step size: 0.050354309500107775\n",
            "Loss on this step: 4013.4377825952765, Loss on the last accepted step: 4024.073783956449, Step size: 0.050354309500107775\n",
            "Loss on this step: 3993.2899729160954, Loss on the last accepted step: 4013.4377825952765, Step size: 0.050354309500107775\n",
            "Loss on this step: 3991.5955432305427, Loss on the last accepted step: 3993.2899729160954, Step size: 0.050354309500107775\n",
            "Loss on this step: 3967.1534309102253, Loss on the last accepted step: 3991.5955432305427, Step size: 0.050354309500107775\n",
            "Loss on this step: 3955.2358789898085, Loss on the last accepted step: 3967.1534309102253, Step size: 0.050354309500107775\n",
            "Loss on this step: 3945.8115241954747, Loss on the last accepted step: 3955.2358789898085, Step size: 0.050354309500107775\n",
            "Loss on this step: 3938.993496803196, Loss on the last accepted step: 3945.8115241954747, Step size: 0.050354309500107775\n",
            "Loss on this step: 3932.2072409015923, Loss on the last accepted step: 3938.993496803196, Step size: 0.050354309500107775\n",
            "Loss on this step: 3927.5810211213343, Loss on the last accepted step: 3932.2072409015923, Step size: 0.050354309500107775\n",
            "Loss on this step: 3923.63818940187, Loss on the last accepted step: 3927.5810211213343, Step size: 0.1762400832503772\n",
            "Loss on this step: 3914.9324701665746, Loss on the last accepted step: 3923.63818940187, Step size: 0.1762400832503772\n",
            "Loss on this step: 3901.5495636602623, Loss on the last accepted step: 3914.9324701665746, Step size: 0.1762400832503772\n",
            "Loss on this step: 3897.623328024606, Loss on the last accepted step: 3901.5495636602623, Step size: 0.1762400832503772\n",
            "Loss on this step: 3893.760671877282, Loss on the last accepted step: 3897.623328024606, Step size: 0.1762400832503772\n",
            "Loss on this step: 3891.2895880857, Loss on the last accepted step: 3893.760671877282, Step size: 0.1762400832503772\n",
            "Loss on this step: 3890.370659861142, Loss on the last accepted step: 3891.2895880857, Step size: 0.1762400832503772\n",
            "Loss on this step: 3889.5465738192943, Loss on the last accepted step: 3890.370659861142, Step size: 0.1762400832503772\n",
            "Loss on this step: 3889.12491377113, Loss on the last accepted step: 3889.5465738192943, Step size: 0.1762400832503772\n",
            "Loss on this step: 3888.859340256517, Loss on the last accepted step: 3889.12491377113, Step size: 0.1762400832503772\n",
            "Loss on this step: 3888.7110142212987, Loss on the last accepted step: 3888.859340256517, Step size: 0.1762400832503772\n",
            "Loss on this step: 3888.6090251263413, Loss on the last accepted step: 3888.7110142212987, Step size: 0.1762400832503772\n",
            "Loss on this step: 3888.5466044611317, Loss on the last accepted step: 3888.6090251263413, Step size: 0.1762400832503772\n",
            "Loss on this step: 3888.5164905096526, Loss on the last accepted step: 3888.5466044611317, Step size: 0.1762400832503772\n",
            "Loss on this step: 3888.49339562081, Loss on the last accepted step: 3888.5164905096526, Step size: 0.1762400832503772\n",
            "Loss on this step: 3888.481485523379, Loss on the last accepted step: 3888.49339562081, Step size: 0.1762400832503772\n",
            "Loss on this step: 3888.47520988601, Loss on the last accepted step: 3888.481485523379, Step size: 0.1762400832503772\n",
            "Loss on this step: 3888.471888656517, Loss on the last accepted step: 3888.47520988601, Step size: 0.1762400832503772\n",
            "Loss on this step: 3888.469498275625, Loss on the last accepted step: 3888.471888656517, Step size: 0.1762400832503772\n",
            "Loss on this step: 3888.467776387179, Loss on the last accepted step: 3888.469498275625, Step size: 0.1762400832503772\n",
            "Loss on this step: 3888.466746887981, Loss on the last accepted step: 3888.467776387179, Step size: 0.1762400832503772\n",
            "Loss on this step: 3888.466187562192, Loss on the last accepted step: 3888.466746887981, Step size: 0.6168402913763202\n",
            "Loss on this step: 3888.4652306083212, Loss on the last accepted step: 3888.466187562192, Step size: 0.6168402913763202\n",
            "Loss on this step: 3888.4650918807515, Loss on the last accepted step: 3888.4652306083212, Step size: 0.6168402913763202\n",
            "Loss on this step: 3888.4650723483164, Loss on the last accepted step: 3888.4650918807515, Step size: 0.6168402913763202\n",
            "Loss on this step: 3888.4650700940715, Loss on the last accepted step: 3888.4650723483164, Step size: 0.6168402913763202\n",
            "Loss on this step: 3888.4650698425908, Loss on the last accepted step: 3888.4650700940715, Step size: 0.6168402913763202\n",
            "Loss on this step: 3888.465069807734, Loss on the last accepted step: 3888.4650698425908, Step size: 0.6168402913763202\n",
            "Loss on this step: 3888.465069802795, Loss on the last accepted step: 3888.465069807734, Step size: 0.6168402913763202\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-14 00:41:09,014 INFO jaxlogit.mixed_logit: Optimization finished, success = True, final loglike = -3888.47, final gradient max = 3.10e-05, norm = 8.86e-05.\n",
            "2025-07-14 00:41:09,015 INFO jaxlogit.mixed_logit: Calculating gradient of individual log-likelihood contributions\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss on this step: 3888.4650698021405, Loss on the last accepted step: 3888.465069802795, Step size: 0.6168402913763202\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-14 00:41:11,288 INFO jaxlogit.mixed_logit: Calculating H_inv\n",
            "2025-07-14 00:41:14,863 INFO jaxlogit._choice_model: Post fit processing\n",
            "2025-07-14 00:41:15,476 INFO jaxlogit._choice_model: Optimization terminated successfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Message: \n",
            "    Iterations: 124\n",
            "    Function evaluations: 137\n",
            "Estimation time= 25.4 seconds\n",
            "---------------------------------------------------------------------------\n",
            "Coefficient              Estimate      Std.Err.         z-val         P>|z|\n",
            "---------------------------------------------------------------------------\n",
            "pf                     -0.9972113     0.0378219   -26.3660027     3.71e-142 ***\n",
            "cl                     -0.2196750     0.0255168    -8.6090387      1.02e-17 ***\n",
            "loc                     2.2902239     0.1263057    18.1323892      7.19e-71 ***\n",
            "wk                      1.6943088     0.0961498    17.6215600      3.63e-67 ***\n",
            "tod                    -9.6752534     0.3350900   -28.8735984     9.73e-168 ***\n",
            "seas                   -9.6962259     0.3246516   -29.8665562     2.65e-178 ***\n",
            "sd.pf                   0.2207270     0.0191064    11.5525033         2e-30 ***\n",
            "sd.cl                   0.4115603     0.0254614    16.1640699       4.2e-57 ***\n",
            "sd.loc                  1.7840292     0.1181321    15.1019799      2.98e-50 ***\n",
            "sd.wk                   1.2296232     0.0943604    13.0311319      4.24e-38 ***\n",
            "sd.tod                  2.2757070     0.1737687    13.0961874      1.86e-38 ***\n",
            "sd.seas                 1.4862389     0.1638379     9.0713986      1.75e-19 ***\n",
            "---------------------------------------------------------------------------\n",
            "Significance:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
            "\n",
            "Log-Likelihood= -3888.465\n",
            "AIC= 7800.930\n",
            "BIC= 7877.349\n"
          ]
        }
      ],
      "source": [
        "# Note the sd. variables in jaxlogit are softplus transformed by default such that they are always positive. To compare these to xlogits results at https://github.com/arteagac/xlogit/blob/master/examples/mixed_logit_model.ipynb\n",
        "# use jax.nn.softplus(params) for non-asserted sd. params. Or run w/o softplus:\n",
        "model = MixedLogit()\n",
        "\n",
        "config = ConfigData(\n",
        "    force_positive_chol_diag=False,\n",
        "    panels=df['id'],\n",
        "    n_draws=600,\n",
        ")\n",
        "\n",
        "res = model.fit(\n",
        "    X=df[varnames],\n",
        "    y=df['choice'],\n",
        "    varnames=varnames,\n",
        "    ids=df['chid'],\n",
        "    alts=df['alt'],\n",
        "    randvars={'pf': 'n', 'cl': 'n', 'loc': 'n', 'wk': 'n', 'tod': 'n', 'seas': 'n'},\n",
        "    config=config\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9gxNL0XePRc"
      },
      "source": [
        "## Fishing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Innu2ypbmKui"
      },
      "source": [
        "This example illustrates the estimation of a Mixed Logit model for choices of 1,182 individuals for sport fishing modes using `xlogit`. The goal is to analyze the market shares of four alternatives (i.e., beach, pier, boat, and charter) based on their cost and fish catch rate. [Cameron (2005)](http://cameron.econ.ucdavis.edu/mmabook/mma.html) provides additional details about this dataset. The following code illustrates how to use `xlogit` to estimate the model parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqBJWh8eOQDp"
      },
      "source": [
        "### Read data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqCeQywgozbK"
      },
      "source": [
        "The data to be analyzed can be imported to Python using any preferred method. In this example, the data in CSV format was imported using the popular `pandas` Python package. However, it is worth highlighting that `xlogit` does not depend on the `pandas` package, as `xlogit` can take any array-like structure as input. This represents an additional advantage because `xlogit` can be used with any preferred dataframe library, and not only with `pandas`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "9jDr3PIveaG8",
        "outputId": "490ab844-e202-4ba3-cdcf-cf64c54fc698"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>alt</th>\n",
              "      <th>choice</th>\n",
              "      <th>income</th>\n",
              "      <th>price</th>\n",
              "      <th>catch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>beach</td>\n",
              "      <td>0</td>\n",
              "      <td>7083.33170</td>\n",
              "      <td>157.930</td>\n",
              "      <td>0.0678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>boat</td>\n",
              "      <td>0</td>\n",
              "      <td>7083.33170</td>\n",
              "      <td>157.930</td>\n",
              "      <td>0.2601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>charter</td>\n",
              "      <td>1</td>\n",
              "      <td>7083.33170</td>\n",
              "      <td>182.930</td>\n",
              "      <td>0.5391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>pier</td>\n",
              "      <td>0</td>\n",
              "      <td>7083.33170</td>\n",
              "      <td>157.930</td>\n",
              "      <td>0.0503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>beach</td>\n",
              "      <td>0</td>\n",
              "      <td>1249.99980</td>\n",
              "      <td>15.114</td>\n",
              "      <td>0.1049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4723</th>\n",
              "      <td>1181</td>\n",
              "      <td>pier</td>\n",
              "      <td>0</td>\n",
              "      <td>416.66668</td>\n",
              "      <td>36.636</td>\n",
              "      <td>0.4522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4724</th>\n",
              "      <td>1182</td>\n",
              "      <td>beach</td>\n",
              "      <td>0</td>\n",
              "      <td>6250.00130</td>\n",
              "      <td>339.890</td>\n",
              "      <td>0.2537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4725</th>\n",
              "      <td>1182</td>\n",
              "      <td>boat</td>\n",
              "      <td>1</td>\n",
              "      <td>6250.00130</td>\n",
              "      <td>235.436</td>\n",
              "      <td>0.6817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4726</th>\n",
              "      <td>1182</td>\n",
              "      <td>charter</td>\n",
              "      <td>0</td>\n",
              "      <td>6250.00130</td>\n",
              "      <td>260.436</td>\n",
              "      <td>2.3014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4727</th>\n",
              "      <td>1182</td>\n",
              "      <td>pier</td>\n",
              "      <td>0</td>\n",
              "      <td>6250.00130</td>\n",
              "      <td>339.890</td>\n",
              "      <td>0.1498</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4728 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id      alt  choice      income    price   catch\n",
              "0        1    beach       0  7083.33170  157.930  0.0678\n",
              "1        1     boat       0  7083.33170  157.930  0.2601\n",
              "2        1  charter       1  7083.33170  182.930  0.5391\n",
              "3        1     pier       0  7083.33170  157.930  0.0503\n",
              "4        2    beach       0  1249.99980   15.114  0.1049\n",
              "...    ...      ...     ...         ...      ...     ...\n",
              "4723  1181     pier       0   416.66668   36.636  0.4522\n",
              "4724  1182    beach       0  6250.00130  339.890  0.2537\n",
              "4725  1182     boat       1  6250.00130  235.436  0.6817\n",
              "4726  1182  charter       0  6250.00130  260.436  2.3014\n",
              "4727  1182     pier       0  6250.00130  339.890  0.1498\n",
              "\n",
              "[4728 rows x 6 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"https://raw.github.com/arteagac/xlogit/master/examples/data/fishing_long.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rffV7cx8ORpP"
      },
      "source": [
        "### Fit model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFcmUz8Xo6UT"
      },
      "source": [
        "Once the data is in the `Python` environment, `xlogit` can be used to fit the model, as shown below. The `MultinomialLogit` class is imported from `xlogit`, and its constructor is used to initialize a new model. The `fit` method estimates the model using the input data and estimation criteria provided as arguments to the method's call. The arguments of the `fit` methods are described in [`xlogit`'s documentation](https://https://xlogit.readthedocs.io/en/latest/api/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIZwBe0zedfh",
        "outputId": "46561fa7-3339-4fb1-9c93-3c2a4a646c96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-14 00:42:22,862 INFO jaxlogit.mixed_logit: Starting data preparation, including generation of 2000 random draws for each random variable and observation.\n",
            "2025-07-14 00:42:24,039 INFO jaxlogit.mixed_logit: Shape of draws: (1182, 2, 2000), number of draws: 2000\n",
            "2025-07-14 00:42:24,040 INFO jaxlogit.mixed_logit: Shape of Xdf: (1182, 3, 0), shape of Xdr: (1182, 3, 2)\n",
            "2025-07-14 00:42:24,040 INFO jaxlogit.mixed_logit: Compiling log-likelihood function.\n",
            "2025-07-14 00:42:24,193 INFO jaxlogit.mixed_logit: Compilation finished, init neg_loglike = 2342.17, params= [(np.str_('price'), Array(0.1, dtype=float64)), (np.str_('catch'), Array(0.1, dtype=float64)), (np.str_('sd.price'), Array(0.1, dtype=float64)), (np.str_('sd.catch'), Array(0.1, dtype=float64))]\n",
            "2025-07-14 00:42:24,194 INFO jaxlogit._optimize: Running minimization with method trust-region\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss on this step: 2342.172165320885, Loss on the last accepted step: 0.0, Step size: 1.0\n",
            "Loss on this step: 346690.4235844468, Loss on the last accepted step: 2342.172165320885, Step size: 0.25\n",
            "Loss on this step: 343586.74230861163, Loss on the last accepted step: 2342.172165320885, Step size: 0.0625\n",
            "Loss on this step: 340345.4710080018, Loss on the last accepted step: 2342.172165320885, Step size: 0.015625\n",
            "Loss on this step: 186655.5449715904, Loss on the last accepted step: 2342.172165320885, Step size: 0.00390625\n",
            "Loss on this step: 26460.878950393257, Loss on the last accepted step: 2342.172165320885, Step size: 0.0009765625\n",
            "Loss on this step: 2188.526926627541, Loss on the last accepted step: 2342.172165320885, Step size: 0.0009765625\n",
            "Loss on this step: 2132.948497166779, Loss on the last accepted step: 2188.526926627541, Step size: 0.00341796875\n",
            "Loss on this step: 3066.1216490150573, Loss on the last accepted step: 2132.948497166779, Step size: 0.0008544921875\n",
            "Loss on this step: 2079.176253726482, Loss on the last accepted step: 2132.948497166779, Step size: 0.0008544921875\n",
            "Loss on this step: 2036.7797201817684, Loss on the last accepted step: 2079.176253726482, Step size: 0.00299072265625\n",
            "Loss on this step: 1859.8037731492846, Loss on the last accepted step: 2036.7797201817684, Step size: 0.010467529296875\n",
            "Loss on this step: 406872.2751080955, Loss on the last accepted step: 1859.8037731492846, Step size: 0.00261688232421875\n",
            "Loss on this step: 115121.02757443895, Loss on the last accepted step: 1859.8037731492846, Step size: 0.0006542205810546875\n",
            "Loss on this step: 3356.0971093072694, Loss on the last accepted step: 1859.8037731492846, Step size: 0.00016355514526367188\n",
            "Loss on this step: 1746.5202468820169, Loss on the last accepted step: 1859.8037731492846, Step size: 0.0005724430084228516\n",
            "Loss on this step: 41317.99445607107, Loss on the last accepted step: 1746.5202468820169, Step size: 0.0001431107521057129\n",
            "Loss on this step: 1853.6191969577224, Loss on the last accepted step: 1746.5202468820169, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1689.3630702920289, Loss on the last accepted step: 1746.5202468820169, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1643.597022825867, Loss on the last accepted step: 1689.3630702920289, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1605.2564724874694, Loss on the last accepted step: 1643.597022825867, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1574.2320585967339, Loss on the last accepted step: 1605.2564724874694, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1550.3218290432135, Loss on the last accepted step: 1574.2320585967339, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1534.5562816820368, Loss on the last accepted step: 1550.3218290432135, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1522.3762037362878, Loss on the last accepted step: 1534.5562816820368, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1512.9697076278776, Loss on the last accepted step: 1522.3762037362878, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1505.4050945858207, Loss on the last accepted step: 1512.9697076278776, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1499.1524713172403, Loss on the last accepted step: 1505.4050945858207, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1493.79725252576, Loss on the last accepted step: 1499.1524713172403, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1489.1084280633347, Loss on the last accepted step: 1493.79725252576, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1484.9729546686601, Loss on the last accepted step: 1489.1084280633347, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1481.2208064236256, Loss on the last accepted step: 1484.9729546686601, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1477.8464918824898, Loss on the last accepted step: 1481.2208064236256, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1474.7160671199383, Loss on the last accepted step: 1477.8464918824898, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1471.8627259244358, Loss on the last accepted step: 1474.7160671199383, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1469.1754354540694, Loss on the last accepted step: 1471.8627259244358, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1466.7019293107069, Loss on the last accepted step: 1469.1754354540694, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1464.3470132920038, Loss on the last accepted step: 1466.7019293107069, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1462.1632974596055, Loss on the last accepted step: 1464.3470132920038, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1460.0673077824151, Loss on the last accepted step: 1462.1632974596055, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1458.1123409650322, Loss on the last accepted step: 1460.0673077824151, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1456.224020735849, Loss on the last accepted step: 1458.1123409650322, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1454.4544501478313, Loss on the last accepted step: 1456.224020735849, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1452.736562919023, Loss on the last accepted step: 1454.4544501478313, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1451.1204461610948, Loss on the last accepted step: 1452.736562919023, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1449.5450583072352, Loss on the last accepted step: 1451.1204461610948, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1448.0581519436332, Loss on the last accepted step: 1449.5450583072352, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1446.603752392236, Loss on the last accepted step: 1448.0581519436332, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1445.2272021854126, Loss on the last accepted step: 1446.603752392236, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1443.8768618190838, Loss on the last accepted step: 1445.2272021854126, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1442.5957022942923, Loss on the last accepted step: 1443.8768618190838, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1441.3358485685255, Loss on the last accepted step: 1442.5957022942923, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1440.137994632326, Loss on the last accepted step: 1441.3358485685255, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1438.9575719624536, Loss on the last accepted step: 1440.137994632326, Step size: 3.577768802642822e-05\n",
            "Loss on this step: 1437.8331190445024, Loss on the last accepted step: 1438.9575719624536, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1433.74364382346, Loss on the last accepted step: 1437.8331190445024, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1430.3988961260359, Loss on the last accepted step: 1433.74364382346, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1427.607608002228, Loss on the last accepted step: 1430.3988961260359, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1425.2901575129558, Loss on the last accepted step: 1427.607608002228, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1423.184009198222, Loss on the last accepted step: 1425.2901575129558, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1421.347078778109, Loss on the last accepted step: 1423.184009198222, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1419.6302985477141, Loss on the last accepted step: 1421.347078778109, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1418.0920301343317, Loss on the last accepted step: 1419.6302985477141, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1416.629755922395, Loss on the last accepted step: 1418.0920301343317, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1415.297717225932, Loss on the last accepted step: 1416.629755922395, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1414.017280858901, Loss on the last accepted step: 1415.297717225932, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1412.8377226468403, Loss on the last accepted step: 1414.017280858901, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1411.6949241140596, Loss on the last accepted step: 1412.8377226468403, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1410.633528824731, Loss on the last accepted step: 1411.6949241140596, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1409.5992147061474, Loss on the last accepted step: 1410.633528824731, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1408.6325677447203, Loss on the last accepted step: 1409.5992147061474, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1407.6863598429263, Loss on the last accepted step: 1408.6325677447203, Step size: 0.00012522190809249878\n",
            "Loss on this step: 1406.797679749392, Loss on the last accepted step: 1407.6863598429263, Step size: 0.00043827667832374573\n",
            "Loss on this step: 1403.6882612011332, Loss on the last accepted step: 1406.797679749392, Step size: 0.00043827667832374573\n",
            "Loss on this step: 1401.2678801363986, Loss on the last accepted step: 1403.6882612011332, Step size: 0.00043827667832374573\n",
            "Loss on this step: 1399.297549266983, Loss on the last accepted step: 1401.2678801363986, Step size: 0.00043827667832374573\n",
            "Loss on this step: 1397.6796732158632, Loss on the last accepted step: 1399.297549266983, Step size: 0.00043827667832374573\n",
            "Loss on this step: 1396.212297308486, Loss on the last accepted step: 1397.6796732158632, Step size: 0.00043827667832374573\n",
            "Loss on this step: 1394.9364030119555, Loss on the last accepted step: 1396.212297308486, Step size: 0.00043827667832374573\n",
            "Loss on this step: 1393.7425283258017, Loss on the last accepted step: 1394.9364030119555, Step size: 0.00043827667832374573\n",
            "Loss on this step: 1392.6729511241801, Loss on the last accepted step: 1393.7425283258017, Step size: 0.00043827667832374573\n",
            "Loss on this step: 1391.653836661209, Loss on the last accepted step: 1392.6729511241801, Step size: 0.00043827667832374573\n",
            "Loss on this step: 1390.7245655192623, Loss on the last accepted step: 1391.653836661209, Step size: 0.00043827667832374573\n",
            "Loss on this step: 1389.8288953788563, Loss on the last accepted step: 1390.7245655192623, Step size: 0.00043827667832374573\n",
            "Loss on this step: 1389.0025488170804, Loss on the last accepted step: 1389.8288953788563, Step size: 0.00043827667832374573\n",
            "Loss on this step: 1388.1997629019252, Loss on the last accepted step: 1389.0025488170804, Step size: 0.00043827667832374573\n",
            "Loss on this step: 1387.4528580572787, Loss on the last accepted step: 1388.1997629019252, Step size: 0.00153396837413311\n",
            "Loss on this step: 1384.8984595329434, Loss on the last accepted step: 1387.4528580572787, Step size: 0.00153396837413311\n",
            "Loss on this step: 1382.9744649852983, Loss on the last accepted step: 1384.8984595329434, Step size: 0.00153396837413311\n",
            "Loss on this step: 1381.4302147620501, Loss on the last accepted step: 1382.9744649852983, Step size: 0.00153396837413311\n",
            "Loss on this step: 1380.160643115728, Loss on the last accepted step: 1381.4302147620501, Step size: 0.00153396837413311\n",
            "Loss on this step: 1379.007683329709, Loss on the last accepted step: 1380.160643115728, Step size: 0.00153396837413311\n",
            "Loss on this step: 1378.0013374986152, Loss on the last accepted step: 1379.007683329709, Step size: 0.00153396837413311\n",
            "Loss on this step: 1377.0568534252495, Loss on the last accepted step: 1378.0013374986152, Step size: 0.00153396837413311\n",
            "Loss on this step: 1376.2069501680062, Loss on the last accepted step: 1377.0568534252495, Step size: 0.00153396837413311\n",
            "Loss on this step: 1375.3943465945565, Loss on the last accepted step: 1376.2069501680062, Step size: 0.00153396837413311\n",
            "Loss on this step: 1374.6500980933156, Loss on the last accepted step: 1375.3943465945565, Step size: 0.005368889309465885\n",
            "Loss on this step: 1372.1622601816566, Loss on the last accepted step: 1374.6500980933156, Step size: 0.005368889309465885\n",
            "Loss on this step: 1370.3563008560532, Loss on the last accepted step: 1372.1622601816566, Step size: 0.005368889309465885\n",
            "Loss on this step: 1368.9181403531757, Loss on the last accepted step: 1370.3563008560532, Step size: 0.005368889309465885\n",
            "Loss on this step: 1367.7104493094307, Loss on the last accepted step: 1368.9181403531757, Step size: 0.005368889309465885\n",
            "Loss on this step: 1366.595951394262, Loss on the last accepted step: 1367.7104493094307, Step size: 0.018791112583130598\n",
            "Loss on this step: 1363.1454127824745, Loss on the last accepted step: 1366.595951394262, Step size: 0.018791112583130598\n",
            "Loss on this step: 1360.6560150490818, Loss on the last accepted step: 1363.1454127824745, Step size: 0.018791112583130598\n",
            "Loss on this step: 1358.425540548714, Loss on the last accepted step: 1360.6560150490818, Step size: 0.0657688940409571\n",
            "Loss on this step: 1351.3464646664424, Loss on the last accepted step: 1358.425540548714, Step size: 0.0657688940409571\n",
            "Loss on this step: 1348.7155796224872, Loss on the last accepted step: 1351.3464646664424, Step size: 0.0657688940409571\n",
            "Loss on this step: 1314.8870423725382, Loss on the last accepted step: 1348.7155796224872, Step size: 0.0657688940409571\n",
            "Loss on this step: 1310.7231499332656, Loss on the last accepted step: 1314.8870423725382, Step size: 0.0657688940409571\n",
            "Loss on this step: 1309.6324911370434, Loss on the last accepted step: 1310.7231499332656, Step size: 0.23019112914334983\n",
            "Loss on this step: 1306.912056097575, Loss on the last accepted step: 1309.6324911370434, Step size: 0.8056689520017244\n",
            "Loss on this step: 1301.093753072791, Loss on the last accepted step: 1306.912056097575, Step size: 0.8056689520017244\n",
            "Loss on this step: 1300.7667221797408, Loss on the last accepted step: 1301.093753072791, Step size: 0.8056689520017244\n",
            "Loss on this step: 1300.6623264485384, Loss on the last accepted step: 1300.7667221797408, Step size: 0.8056689520017244\n",
            "Loss on this step: 1300.9839261725097, Loss on the last accepted step: 1300.6623264485384, Step size: 0.2014172380004311\n",
            "Loss on this step: 1300.6028031979922, Loss on the last accepted step: 1300.6623264485384, Step size: 0.2014172380004311\n",
            "Loss on this step: 1300.5909115560798, Loss on the last accepted step: 1300.6028031979922, Step size: 0.7049603330015088\n",
            "Loss on this step: 1300.5827334571486, Loss on the last accepted step: 1300.5909115560798, Step size: 2.467361165505281\n",
            "Loss on this step: 1300.5817139290157, Loss on the last accepted step: 1300.5827334571486, Step size: 8.635764079268483\n",
            "Loss on this step: 1300.5817132828292, Loss on the last accepted step: 1300.5817139290157, Step size: 30.22517427743969\n",
            "Loss on this step: 1300.5817132549764, Loss on the last accepted step: 1300.5817132828292, Step size: 105.78810997103892\n",
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549764, Step size: 370.2583848986362\n",
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549464, Step size: 92.56459622465906\n",
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549464, Step size: 23.141149056164764\n",
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549464, Step size: 5.785287264041191\n",
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549464, Step size: 1.4463218160102977\n",
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549464, Step size: 0.36158045400257444\n",
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549464, Step size: 0.09039511350064361\n",
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549464, Step size: 0.022598778375160902\n",
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549464, Step size: 0.005649694593790226\n",
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549464, Step size: 0.0014124236484475564\n",
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549464, Step size: 0.0003531059121118891\n",
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549464, Step size: 8.827647802797227e-05\n",
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549464, Step size: 2.206911950699307e-05\n",
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549464, Step size: 5.517279876748267e-06\n",
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549464, Step size: 1.3793199691870668e-06\n",
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549464, Step size: 3.448299922967667e-07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-14 00:42:32,323 INFO jaxlogit.mixed_logit: Optimization finished, success = True, final loglike = -1300.58, final gradient max = -4.50e-07, norm = 1.55e-04.\n",
            "2025-07-14 00:42:32,324 INFO jaxlogit.mixed_logit: Calculating gradient of individual log-likelihood contributions\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss on this step: 1300.5817132549464, Loss on the last accepted step: 1300.5817132549464, Step size: 8.620749807419167e-08\n",
            "Loss on this step: 1300.5817132549462, Loss on the last accepted step: 1300.5817132549464, Step size: 3.017262432596709e-07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-14 00:42:32,994 INFO jaxlogit.mixed_logit: Calculating H_inv\n",
            "2025-07-14 00:42:33,779 INFO jaxlogit._choice_model: Post fit processing\n",
            "2025-07-14 00:42:34,077 INFO jaxlogit._choice_model: Optimization terminated successfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Message: \n",
            "    Iterations: 111\n",
            "    Function evaluations: 139\n",
            "Estimation time= 11.2 seconds\n",
            "---------------------------------------------------------------------------\n",
            "Coefficient              Estimate      Std.Err.         z-val         P>|z|\n",
            "---------------------------------------------------------------------------\n",
            "price                  -0.0272479     0.0022848   -11.9259324      4.83e-31 ***\n",
            "catch                   1.3258108     0.1738880     7.6245081      5.01e-14 ***\n",
            "sd.price               -4.5741720     0.2085573   -21.9324510      9.97e-90 ***\n",
            "sd.catch                1.3316625     0.4742604     2.8078723       0.00507 ** \n",
            "---------------------------------------------------------------------------\n",
            "Significance:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
            "\n",
            "Log-Likelihood= -1300.582\n",
            "AIC= 2609.163\n",
            "BIC= 2629.463\n"
          ]
        }
      ],
      "source": [
        "varnames = ['price', 'catch']\n",
        "model = MixedLogit()\n",
        "\n",
        "config = ConfigData(\n",
        "    n_draws=2000,  # Note using 1000 draws here leads to sd.catch going to zero, need more draws to find minimum at positive stddev\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X=df[varnames],\n",
        "    y=df['choice'],\n",
        "    varnames=varnames,\n",
        "    alts=df['alt'],\n",
        "    ids=df['id'],\n",
        "    randvars={'price': 'n', 'catch': 'n'},\n",
        "    config=config\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([0.010262  , 1.56597382], dtype=float64)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# sd. vals agree with xlogit results except for sign of sd.catch, which is due to xlogit not restricting the sd devs to positive parameters and the log-likelihood being symmetric wrt to sign of normal std dev for non-correlated parameters. \n",
        "jax.nn.softplus(model.coeff_[len(model._rvidx):])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWU80LmcODPY"
      },
      "source": [
        "## Car Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1zgiBGKouPr"
      },
      "source": [
        "The fourth example uses a stated preference panel dataset for choice of car. Three alternatives are considered, with upto 6 choice situations per individual. This again is an unbalanced panel with responses of some individuals less than 6 situations. The dataset contains 8 explanaotry variables: price, operating cost, range, and binary indicators to indicate whether the car is electric, hybrid, and if performance is high or medium respectively. This dataset was taken from Kenneth Train's MATLAB codes for estimation of Mixed Logit models as shown in this link: https://eml.berkeley.edu/Software/abstracts/train1006mxlmsl.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoSyQfjqkNU3"
      },
      "source": [
        "### Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8AAMruCj8tt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"https://raw.github.com/arteagac/xlogit/master/examples/data/car100_long.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HY7mT__Lj5b"
      },
      "source": [
        "Since price and operating cost need to be estimated with negative coefficients, we reverse the variable signs in the dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "TQ33gsZZLkP5",
        "outputId": "7acb35c7-38b4-4017-b0ac-ae1dd2ef8b59"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>person_id</th>\n",
              "      <th>choice_id</th>\n",
              "      <th>alt</th>\n",
              "      <th>choice</th>\n",
              "      <th>price</th>\n",
              "      <th>opcost</th>\n",
              "      <th>range</th>\n",
              "      <th>ev</th>\n",
              "      <th>gas</th>\n",
              "      <th>hybrid</th>\n",
              "      <th>hiperf</th>\n",
              "      <th>medhiperf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-4.6763</td>\n",
              "      <td>-47.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-5.7209</td>\n",
              "      <td>-27.43</td>\n",
              "      <td>1.3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-8.7960</td>\n",
              "      <td>-32.41</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.3768</td>\n",
              "      <td>-4.89</td>\n",
              "      <td>1.3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-9.0336</td>\n",
              "      <td>-30.19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4447</th>\n",
              "      <td>100</td>\n",
              "      <td>1483</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.8036</td>\n",
              "      <td>-14.45</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4448</th>\n",
              "      <td>100</td>\n",
              "      <td>1483</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.9360</td>\n",
              "      <td>-54.76</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4449</th>\n",
              "      <td>100</td>\n",
              "      <td>1484</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-2.4054</td>\n",
              "      <td>-50.57</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4450</th>\n",
              "      <td>100</td>\n",
              "      <td>1484</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-5.2795</td>\n",
              "      <td>-21.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4451</th>\n",
              "      <td>100</td>\n",
              "      <td>1484</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-6.0705</td>\n",
              "      <td>-25.41</td>\n",
              "      <td>1.4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4452 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      person_id  choice_id  alt  choice   price  opcost  range  ev  gas  \\\n",
              "0             1          1    1       0 -4.6763  -47.43    0.0   0    0   \n",
              "1             1          1    2       1 -5.7209  -27.43    1.3   1    0   \n",
              "2             1          1    3       0 -8.7960  -32.41    1.2   1    0   \n",
              "3             1          2    1       1 -3.3768   -4.89    1.3   1    0   \n",
              "4             1          2    2       0 -9.0336  -30.19    0.0   0    0   \n",
              "...         ...        ...  ...     ...     ...     ...    ...  ..  ...   \n",
              "4447        100       1483    2       0 -2.8036  -14.45    1.6   1    0   \n",
              "4448        100       1483    3       0 -1.9360  -54.76    0.0   0    1   \n",
              "4449        100       1484    1       1 -2.4054  -50.57    0.0   0    1   \n",
              "4450        100       1484    2       0 -5.2795  -21.25    0.0   0    0   \n",
              "4451        100       1484    3       0 -6.0705  -25.41    1.4   1    0   \n",
              "\n",
              "      hybrid  hiperf  medhiperf  \n",
              "0          1       0          0  \n",
              "1          0       1          1  \n",
              "2          0       0          1  \n",
              "3          0       1          1  \n",
              "4          1       0          1  \n",
              "...      ...     ...        ...  \n",
              "4447       0       0          0  \n",
              "4448       0       1          1  \n",
              "4449       0       0          0  \n",
              "4450       1       0          1  \n",
              "4451       0       0          0  \n",
              "\n",
              "[4452 rows x 12 columns]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['price'] = -df['price']/10000\n",
        "df['opcost'] = -df['opcost']\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZQf9DFKFE5j"
      },
      "source": [
        "### Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MhfvmWgFCX6",
        "outputId": "28de1f2f-2e7c-46be-aa60-105df6b669bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-14 00:47:49,283 INFO jaxlogit.mixed_logit: Starting data preparation, including generation of 1000 random draws for each random variable and observation.\n",
            "2025-07-14 00:47:49,455 INFO jaxlogit.mixed_logit: Lognormal distributions found for 2 random variables, applying transformation.\n",
            "2025-07-14 00:47:49,456 INFO jaxlogit.mixed_logit: Data contains 100 panels, using segment_sum for panel-wise log-likelihood.\n",
            "2025-07-14 00:47:49,457 INFO jaxlogit.mixed_logit: Shape of draws: (1484, 5, 1000), number of draws: 1000\n",
            "2025-07-14 00:47:49,458 INFO jaxlogit.mixed_logit: Shape of Xdf: (1484, 2, 2), shape of Xdr: (1484, 2, 5)\n",
            "2025-07-14 00:47:49,458 INFO jaxlogit.mixed_logit: Compiling log-likelihood function.\n",
            "2025-07-14 00:47:49,536 INFO jaxlogit.mixed_logit: Compilation finished, init neg_loglike = 1737.11, params= [(np.str_('hiperf'), Array(0.1, dtype=float64)), (np.str_('medhiperf'), Array(0.1, dtype=float64)), (np.str_('price'), Array(0.1, dtype=float64)), (np.str_('opcost'), Array(0.1, dtype=float64)), (np.str_('range'), Array(0.1, dtype=float64)), (np.str_('ev'), Array(0.1, dtype=float64)), (np.str_('hybrid'), Array(0.1, dtype=float64)), (np.str_('sd.price'), Array(0.1, dtype=float64)), (np.str_('sd.opcost'), Array(0.1, dtype=float64)), (np.str_('sd.range'), Array(0.1, dtype=float64)), (np.str_('sd.ev'), Array(0.1, dtype=float64)), (np.str_('sd.hybrid'), Array(0.1, dtype=float64))]\n",
            "2025-07-14 00:47:49,537 INFO jaxlogit._optimize: Running minimization with method trust-region\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss on this step: 1737.1071909985076, Loss on the last accepted step: 0.0, Step size: 1.0\n",
            "Loss on this step: 68618.60002635523, Loss on the last accepted step: 1737.1071909985076, Step size: 0.25\n",
            "Loss on this step: 68461.72816998586, Loss on the last accepted step: 1737.1071909985076, Step size: 0.0625\n",
            "Loss on this step: 66708.17052456817, Loss on the last accepted step: 1737.1071909985076, Step size: 0.015625\n",
            "Loss on this step: 22075.294280744205, Loss on the last accepted step: 1737.1071909985076, Step size: 0.00390625\n",
            "Loss on this step: 1588.862771609761, Loss on the last accepted step: 1737.1071909985076, Step size: 0.00390625\n",
            "Loss on this step: 1546.9474055835506, Loss on the last accepted step: 1588.862771609761, Step size: 0.00390625\n",
            "Loss on this step: 1514.787075048447, Loss on the last accepted step: 1546.9474055835506, Step size: 0.013671875\n",
            "Loss on this step: 1414.7004832575594, Loss on the last accepted step: 1514.787075048447, Step size: 0.0478515625\n",
            "Loss on this step: 68780.82250024064, Loss on the last accepted step: 1414.7004832575594, Step size: 0.011962890625\n",
            "Loss on this step: 67680.96790597663, Loss on the last accepted step: 1414.7004832575594, Step size: 0.00299072265625\n",
            "Loss on this step: 63732.80686660195, Loss on the last accepted step: 1414.7004832575594, Step size: 0.0007476806640625\n",
            "Loss on this step: 41168.83538297873, Loss on the last accepted step: 1414.7004832575594, Step size: 0.000186920166015625\n",
            "Loss on this step: 9543.19119192318, Loss on the last accepted step: 1414.7004832575594, Step size: 4.673004150390625e-05\n",
            "Loss on this step: 1392.9554515340433, Loss on the last accepted step: 1414.7004832575594, Step size: 4.673004150390625e-05\n",
            "Loss on this step: 1391.6499887380965, Loss on the last accepted step: 1392.9554515340433, Step size: 0.00016355514526367188\n",
            "Loss on this step: 1341.325305977864, Loss on the last accepted step: 1391.6499887380965, Step size: 0.0005724430084228516\n",
            "Loss on this step: 1332.771745792609, Loss on the last accepted step: 1341.325305977864, Step size: 0.0005724430084228516\n",
            "Loss on this step: 1331.234209883028, Loss on the last accepted step: 1332.771745792609, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1328.6389115848888, Loss on the last accepted step: 1331.234209883028, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1325.8544294074081, Loss on the last accepted step: 1328.6389115848888, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1324.2660656782512, Loss on the last accepted step: 1325.8544294074081, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1323.058284134509, Loss on the last accepted step: 1324.2660656782512, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1322.0883495309079, Loss on the last accepted step: 1323.058284134509, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1321.30014544093, Loss on the last accepted step: 1322.0883495309079, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1320.5856860919966, Loss on the last accepted step: 1321.30014544093, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1319.9471503676843, Loss on the last accepted step: 1320.5856860919966, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1319.3502041118927, Loss on the last accepted step: 1319.9471503676843, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1318.8023388211006, Loss on the last accepted step: 1319.3502041118927, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1318.2834662356397, Loss on the last accepted step: 1318.8023388211006, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1317.7964994425727, Loss on the last accepted step: 1318.2834662356397, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1317.3297388872766, Loss on the last accepted step: 1317.7964994425727, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1316.88516335501, Loss on the last accepted step: 1317.3297388872766, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1316.456270203196, Loss on the last accepted step: 1316.88516335501, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1316.0435234217093, Loss on the last accepted step: 1316.456270203196, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1315.6431279613114, Loss on the last accepted step: 1316.0435234217093, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1315.2542438474857, Loss on the last accepted step: 1315.6431279613114, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1314.8747013866148, Loss on the last accepted step: 1315.2542438474857, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1314.5025881873032, Loss on the last accepted step: 1314.8747013866148, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1314.1364676931046, Loss on the last accepted step: 1314.5025881873032, Step size: 0.0020035505294799805\n",
            "Loss on this step: 1313.773263326461, Loss on the last accepted step: 1314.1364676931046, Step size: 0.007012426853179932\n",
            "Loss on this step: 1312.7185917587408, Loss on the last accepted step: 1313.773263326461, Step size: 0.02454349398612976\n",
            "Loss on this step: 1308.889207489596, Loss on the last accepted step: 1312.7185917587408, Step size: 0.02454349398612976\n",
            "Loss on this step: 1309.37310263575, Loss on the last accepted step: 1308.889207489596, Step size: 0.00613587349653244\n",
            "Loss on this step: 1308.0545975435848, Loss on the last accepted step: 1308.889207489596, Step size: 0.00613587349653244\n",
            "Loss on this step: 1306.9050084008131, Loss on the last accepted step: 1308.0545975435848, Step size: 0.00613587349653244\n",
            "Loss on this step: 1305.822908131737, Loss on the last accepted step: 1306.9050084008131, Step size: 0.00613587349653244\n",
            "Loss on this step: 1305.3682114214823, Loss on the last accepted step: 1305.822908131737, Step size: 0.00613587349653244\n",
            "Loss on this step: 1304.8830793137702, Loss on the last accepted step: 1305.3682114214823, Step size: 0.00613587349653244\n",
            "Loss on this step: 1304.5961283004021, Loss on the last accepted step: 1304.8830793137702, Step size: 0.00613587349653244\n",
            "Loss on this step: 1304.2305706428715, Loss on the last accepted step: 1304.5961283004021, Step size: 0.00613587349653244\n",
            "Loss on this step: 1304.0058289695053, Loss on the last accepted step: 1304.2305706428715, Step size: 0.00613587349653244\n",
            "Loss on this step: 1303.67188880979, Loss on the last accepted step: 1304.0058289695053, Step size: 0.00613587349653244\n",
            "Loss on this step: 1303.42539747126, Loss on the last accepted step: 1303.67188880979, Step size: 0.00613587349653244\n",
            "Loss on this step: 1303.1606506929068, Loss on the last accepted step: 1303.42539747126, Step size: 0.00613587349653244\n",
            "Loss on this step: 1302.9531242535681, Loss on the last accepted step: 1303.1606506929068, Step size: 0.00613587349653244\n",
            "Loss on this step: 1302.7428024806122, Loss on the last accepted step: 1302.9531242535681, Step size: 0.00613587349653244\n",
            "Loss on this step: 1302.5574022611422, Loss on the last accepted step: 1302.7428024806122, Step size: 0.00613587349653244\n",
            "Loss on this step: 1302.3732601022723, Loss on the last accepted step: 1302.5574022611422, Step size: 0.00613587349653244\n",
            "Loss on this step: 1302.2051725066112, Loss on the last accepted step: 1302.3732601022723, Step size: 0.00613587349653244\n",
            "Loss on this step: 1302.0413629193952, Loss on the last accepted step: 1302.2051725066112, Step size: 0.00613587349653244\n",
            "Loss on this step: 1301.890257831802, Loss on the last accepted step: 1302.0413629193952, Step size: 0.00613587349653244\n",
            "Loss on this step: 1301.7437324803889, Loss on the last accepted step: 1301.890257831802, Step size: 0.00613587349653244\n",
            "Loss on this step: 1301.6074190133081, Loss on the last accepted step: 1301.7437324803889, Step size: 0.00613587349653244\n",
            "Loss on this step: 1301.477344132128, Loss on the last accepted step: 1301.6074190133081, Step size: 0.00613587349653244\n",
            "Loss on this step: 1301.356481801597, Loss on the last accepted step: 1301.477344132128, Step size: 0.00613587349653244\n",
            "Loss on this step: 1301.241389469202, Loss on the last accepted step: 1301.356481801597, Step size: 0.00613587349653244\n",
            "Loss on this step: 1301.1339860192838, Loss on the last accepted step: 1301.241389469202, Step size: 0.00613587349653244\n",
            "Loss on this step: 1301.032619874184, Loss on the last accepted step: 1301.1339860192838, Step size: 0.00613587349653244\n",
            "Loss on this step: 1300.9381633037285, Loss on the last accepted step: 1301.032619874184, Step size: 0.00613587349653244\n",
            "Loss on this step: 1300.848737208423, Loss on the last accepted step: 1300.9381633037285, Step size: 0.00613587349653244\n",
            "Loss on this step: 1300.765026251312, Loss on the last accepted step: 1300.848737208423, Step size: 0.00613587349653244\n",
            "Loss on this step: 1300.6858600563637, Loss on the last accepted step: 1300.765026251312, Step size: 0.00613587349653244\n",
            "Loss on this step: 1300.611793696417, Loss on the last accepted step: 1300.6858600563637, Step size: 0.02147555723786354\n",
            "Loss on this step: 1300.4134776143312, Loss on the last accepted step: 1300.611793696417, Step size: 0.02147555723786354\n",
            "Loss on this step: 1300.2534204357228, Loss on the last accepted step: 1300.4134776143312, Step size: 0.02147555723786354\n",
            "Loss on this step: 1300.0827090173452, Loss on the last accepted step: 1300.2534204357228, Step size: 0.02147555723786354\n",
            "Loss on this step: 1300.0300012577072, Loss on the last accepted step: 1300.0827090173452, Step size: 0.02147555723786354\n",
            "Loss on this step: 1299.4399844633786, Loss on the last accepted step: 1300.0300012577072, Step size: 0.02147555723786354\n",
            "Loss on this step: 1299.3209673617148, Loss on the last accepted step: 1299.4399844633786, Step size: 0.02147555723786354\n",
            "Loss on this step: 1299.213026799926, Loss on the last accepted step: 1299.3209673617148, Step size: 0.02147555723786354\n",
            "Loss on this step: 1299.1942674523139, Loss on the last accepted step: 1299.213026799926, Step size: 0.02147555723786354\n",
            "Loss on this step: 1299.095495309067, Loss on the last accepted step: 1299.1942674523139, Step size: 0.02147555723786354\n",
            "Loss on this step: 1299.0421981808313, Loss on the last accepted step: 1299.095495309067, Step size: 0.02147555723786354\n",
            "Loss on this step: 1298.9975035609311, Loss on the last accepted step: 1299.0421981808313, Step size: 0.02147555723786354\n",
            "Loss on this step: 1298.9566714046036, Loss on the last accepted step: 1298.9975035609311, Step size: 0.02147555723786354\n",
            "Loss on this step: 1298.9126490059468, Loss on the last accepted step: 1298.9566714046036, Step size: 0.02147555723786354\n",
            "Loss on this step: 1298.870388952841, Loss on the last accepted step: 1298.9126490059468, Step size: 0.02147555723786354\n",
            "Loss on this step: 1298.8386833033887, Loss on the last accepted step: 1298.870388952841, Step size: 0.02147555723786354\n",
            "Loss on this step: 1298.8029179124844, Loss on the last accepted step: 1298.8386833033887, Step size: 0.02147555723786354\n",
            "Loss on this step: 1298.7738933023138, Loss on the last accepted step: 1298.8029179124844, Step size: 0.02147555723786354\n",
            "Loss on this step: 1298.7438283599863, Loss on the last accepted step: 1298.7738933023138, Step size: 0.02147555723786354\n",
            "Loss on this step: 1298.7138797196194, Loss on the last accepted step: 1298.7438283599863, Step size: 0.02147555723786354\n",
            "Loss on this step: 1298.6882947483232, Loss on the last accepted step: 1298.7138797196194, Step size: 0.02147555723786354\n",
            "Loss on this step: 1298.658801989955, Loss on the last accepted step: 1298.6882947483232, Step size: 0.02147555723786354\n",
            "Loss on this step: 1298.63699469961, Loss on the last accepted step: 1298.658801989955, Step size: 0.02147555723786354\n",
            "Loss on this step: 1298.6120880382055, Loss on the last accepted step: 1298.63699469961, Step size: 0.02147555723786354\n",
            "Loss on this step: 1298.5928986883403, Loss on the last accepted step: 1298.6120880382055, Step size: 0.02147555723786354\n",
            "Loss on this step: 1298.5725916261574, Loss on the last accepted step: 1298.5928986883403, Step size: 0.07516445033252239\n",
            "Loss on this step: 1298.5145287103708, Loss on the last accepted step: 1298.5725916261574, Step size: 0.2630755761638284\n",
            "Loss on this step: 1298.389899617441, Loss on the last accepted step: 1298.5145287103708, Step size: 0.2630755761638284\n",
            "Loss on this step: 1298.2445524994314, Loss on the last accepted step: 1298.389899617441, Step size: 0.2630755761638284\n",
            "Loss on this step: 1298.236581087741, Loss on the last accepted step: 1298.2445524994314, Step size: 0.2630755761638284\n",
            "Loss on this step: 1298.2556180191589, Loss on the last accepted step: 1298.236581087741, Step size: 0.0657688940409571\n",
            "Loss on this step: 1298.2182307617736, Loss on the last accepted step: 1298.236581087741, Step size: 0.0657688940409571\n",
            "Loss on this step: 1298.2118182986983, Loss on the last accepted step: 1298.2182307617736, Step size: 0.0657688940409571\n",
            "Loss on this step: 1298.2058986255208, Loss on the last accepted step: 1298.2118182986983, Step size: 0.0657688940409571\n",
            "Loss on this step: 1298.2017954254516, Loss on the last accepted step: 1298.2058986255208, Step size: 0.0657688940409571\n",
            "Loss on this step: 1298.197398422771, Loss on the last accepted step: 1298.2017954254516, Step size: 0.0657688940409571\n",
            "Loss on this step: 1298.1944242450172, Loss on the last accepted step: 1298.197398422771, Step size: 0.0657688940409571\n",
            "Loss on this step: 1298.1915428524262, Loss on the last accepted step: 1298.1944242450172, Step size: 0.0657688940409571\n",
            "Loss on this step: 1298.1883343346221, Loss on the last accepted step: 1298.1915428524262, Step size: 0.0657688940409571\n",
            "Loss on this step: 1298.184746723918, Loss on the last accepted step: 1298.1883343346221, Step size: 0.0657688940409571\n",
            "Loss on this step: 1298.1833152208178, Loss on the last accepted step: 1298.184746723918, Step size: 0.0657688940409571\n",
            "Loss on this step: 1298.1818521965204, Loss on the last accepted step: 1298.1833152208178, Step size: 0.0657688940409571\n",
            "Loss on this step: 1298.1808301949554, Loss on the last accepted step: 1298.1818521965204, Step size: 0.0657688940409571\n",
            "Loss on this step: 1298.1799984950662, Loss on the last accepted step: 1298.1808301949554, Step size: 0.0657688940409571\n",
            "Loss on this step: 1298.1793951052027, Loss on the last accepted step: 1298.1799984950662, Step size: 0.23019112914334983\n",
            "Loss on this step: 1298.17772469183, Loss on the last accepted step: 1298.1793951052027, Step size: 0.23019112914334983\n",
            "Loss on this step: 1298.176359194305, Loss on the last accepted step: 1298.17772469183, Step size: 0.23019112914334983\n",
            "Loss on this step: 1298.1759260367037, Loss on the last accepted step: 1298.176359194305, Step size: 0.23019112914334983\n",
            "Loss on this step: 1298.1757392326988, Loss on the last accepted step: 1298.1759260367037, Step size: 0.23019112914334983\n",
            "Loss on this step: 1298.1756215572407, Loss on the last accepted step: 1298.1757392326988, Step size: 0.23019112914334983\n",
            "Loss on this step: 1298.1755831655464, Loss on the last accepted step: 1298.1756215572407, Step size: 0.23019112914334983\n",
            "Loss on this step: 1298.1755698850675, Loss on the last accepted step: 1298.1755831655464, Step size: 0.23019112914334983\n",
            "Loss on this step: 1298.1755595421, Loss on the last accepted step: 1298.1755698850675, Step size: 0.23019112914334983\n",
            "Loss on this step: 1298.1755571730164, Loss on the last accepted step: 1298.1755595421, Step size: 0.23019112914334983\n",
            "Loss on this step: 1298.1755555873665, Loss on the last accepted step: 1298.1755571730164, Step size: 0.23019112914334983\n",
            "Loss on this step: 1298.1755549185707, Loss on the last accepted step: 1298.1755555873665, Step size: 0.23019112914334983\n",
            "Loss on this step: 1298.1755544756134, Loss on the last accepted step: 1298.1755549185707, Step size: 0.23019112914334983\n",
            "Loss on this step: 1298.175554234867, Loss on the last accepted step: 1298.1755544756134, Step size: 0.23019112914334983\n",
            "Loss on this step: 1298.1755541355344, Loss on the last accepted step: 1298.175554234867, Step size: 0.8056689520017244\n",
            "Loss on this step: 1298.1755539998635, Loss on the last accepted step: 1298.1755541355344, Step size: 0.8056689520017244\n",
            "Loss on this step: 1298.175553994858, Loss on the last accepted step: 1298.1755539998635, Step size: 0.8056689520017244\n",
            "Loss on this step: 1298.175553994684, Loss on the last accepted step: 1298.175553994858, Step size: 0.8056689520017244\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-14 00:48:02,912 INFO jaxlogit.mixed_logit: Optimization finished, success = True, final loglike = -1298.18, final gradient max = 1.13e-05, norm = 1.13e-05.\n",
            "2025-07-14 00:48:02,914 INFO jaxlogit.mixed_logit: Calculating gradient of individual log-likelihood contributions\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss on this step: 1298.1755539946694, Loss on the last accepted step: 1298.175553994684, Step size: 0.8056689520017244\n",
            "Loss on this step: 1298.1755539946685, Loss on the last accepted step: 1298.1755539946694, Step size: 0.8056689520017244\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-14 00:48:04,350 INFO jaxlogit.mixed_logit: Calculating H_inv\n",
            "2025-07-14 00:48:06,617 INFO jaxlogit._choice_model: Post fit processing\n",
            "2025-07-14 00:48:07,128 INFO jaxlogit._choice_model: Optimization terminated successfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Message: \n",
            "    Iterations: 126\n",
            "    Function evaluations: 137\n",
            "Estimation time= 17.8 seconds\n",
            "---------------------------------------------------------------------------\n",
            "Coefficient              Estimate      Std.Err.         z-val         P>|z|\n",
            "---------------------------------------------------------------------------\n",
            "hiperf                  0.1057373     0.0967950     1.0923846         0.275    \n",
            "medhiperf               0.5713820     0.1004442     5.6885539      1.54e-08 ***\n",
            "price                  -0.7405426     0.1406671    -5.2645058      1.61e-07 ***\n",
            "opcost                  0.0119861     0.0057168     2.0966349        0.0362 *  \n",
            "range                  -0.6709001     0.4032763    -1.6636240        0.0964 .  \n",
            "ev                     -1.5936962     0.3357173    -4.7471372      2.26e-06 ***\n",
            "hybrid                  0.7057744     0.1624134     4.3455432      1.48e-05 ***\n",
            "sd.price                0.4547723     0.1965636     2.3136136        0.0208 *  \n",
            "sd.opcost              -3.2385656     0.1421335   -22.7853721      8.66e-99 ***\n",
            "sd.range               -0.2469161     0.3857246    -0.6401357         0.522    \n",
            "sd.ev                   0.5368134     0.3602553     1.4900915         0.136    \n",
            "sd.hybrid               0.1028975     0.3168303     0.3247715         0.745    \n",
            "---------------------------------------------------------------------------\n",
            "Significance:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
            "\n",
            "Log-Likelihood= -1298.176\n",
            "AIC= 2620.351\n",
            "BIC= 2683.981\n"
          ]
        }
      ],
      "source": [
        "varnames = ['hiperf', 'medhiperf', 'price', 'opcost', 'range', 'ev', 'hybrid'] \n",
        "model = MixedLogit()\n",
        "\n",
        "config = ConfigData(\n",
        "    n_draws = 1000,\n",
        "    panels=df['person_id'],\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X=df[varnames],\n",
        "    y=df['choice'],\n",
        "    varnames=varnames,\n",
        "    alts=df['alt'],\n",
        "    ids=df['choice_id'],\n",
        "    randvars = {'price': 'ln', 'opcost': 'n',  'range': 'ln', 'ev':'n', 'hybrid': 'n'}, \n",
        "    config=config\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([0.94616582, 0.03847054, 0.5772908 , 0.99715057, 0.74591881],      dtype=float64)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jax.nn.softplus(model.coeff_[len(model._rvidx):])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEiWWuCciEJ6"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkfFmr7fiFxc"
      },
      "source": [
        "- Bierlaire, M. (2018). PandasBiogeme: a short introduction. EPFL (Transport and Mobility Laboratory, ENAC).\n",
        "\n",
        "- Brathwaite, T., & Walker, J. L. (2018). Asymmetric, closed-form, finite-parameter models of multinomial choice. Journal of Choice Modelling, 29, 78–112. \n",
        "\n",
        "- Cameron, A. C., & Trivedi, P. K. (2005). Microeconometrics: methods and applications. Cambridge university press.\n",
        "\n",
        "- Croissant, Y. (2020). Estimation of Random Utility Models in R: The mlogit Package. Journal of Statistical Software, 95(1), 1-41.\n",
        "\n",
        "- Revelt, D., & Train, K. (1999). Customer-specific taste parameters and mixed logit. University of California, Berkeley.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "mixed_logit_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.10.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
